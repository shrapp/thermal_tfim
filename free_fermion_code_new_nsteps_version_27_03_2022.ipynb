{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For quantum simulations\n",
    "import qutip\n",
    "from qutip import *\n",
    "# For FFT\n",
    "import scipy\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import cmath\n",
    "import os\n",
    "from fractions import Fraction\n",
    "from numpy import linalg as LA"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize RevTex font\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams[\"font.serif\"] = \"STIX\"\n",
    "mpl.rcParams[\"mathtext.fontset\"] = \"stix\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free fermion model definitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.linalg import logm, expm\n",
    "from sympy import Matrix\n",
    "from pfapack import pfaffian as pf\n",
    "import sympy\n",
    "from IPython.display import display, Latex\n",
    "sympy.init_printing(use_latex='mathjax')\n",
    "from numpy.linalg import matrix_power"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Majorana formalism\n",
    "def h_mat(L,mu,t):\n",
    "    # IMPORTANT:\n",
    "    # t,mu are the H hamiltonian values, they represent the -tZ_1Z_2 and -mu X and so in the Bosonic picture\n",
    "    # From our calculations in the notes we have to multiply both by 4 to get h hamiltonian: h_parameters = 4H_parameters\n",
    "    delta = t\n",
    "    # mu, t, delta as in the notes\n",
    "    mat = np.zeros((2*L,2*L),dtype=np.complex128)\n",
    "    omega_plus = t+delta\n",
    "    omega_minus = t-delta\n",
    "    for i in range(2*L):\n",
    "        if i % 2 == 0:\n",
    "            if i+1 < 2*L:\n",
    "                mat[i,i+1] = mu/2\n",
    "            if i+3 < 2*L:\n",
    "                # there is a factor 1/2 before tunneling terms\n",
    "                mat[i,i+3] = omega_minus/4\n",
    "        else:\n",
    "            if i+1 < 2*L:\n",
    "                # there is a factor 1/2 before tunneling terms\n",
    "                mat[i,i+1] = omega_plus/4\n",
    "    return 1j*(mat-mat.T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Majorana formalism\n",
    "def h_mat_disorder(L,mu,t):\n",
    "    # IMPORTANT:\n",
    "    # t,mu are the H hamiltonian values, they represent the -tX_1X_2 and -mu Z and so in the Bosonic picture\n",
    "    # From our calculations in the notes we have to multiply both by 4 to get h hamiltonian: h_parameters = 4H_parameters\n",
    "    delta = t\n",
    "    # mu, t, delta as in the notes\n",
    "    mat = np.zeros((2*L,2*L),dtype=np.complex128)\n",
    "    omega_plus = t+delta\n",
    "    omega_minus = t+(-1)*delta\n",
    "    cur_ind_mu = 0\n",
    "    cur_ind_om_minus = 0\n",
    "    cur_ind_om_plus = 0\n",
    "    for i in range(2*L):\n",
    "        if i % 2 == 0:\n",
    "            if i+1 < 2*L:\n",
    "                mat[i,i+1] = mu[cur_ind_mu]/2\n",
    "                cur_ind_mu += 1\n",
    "            if i+3 < 2*L:\n",
    "                # there is a factor 1/2 before tunneling terms\n",
    "                mat[i,i+3] = omega_minus[cur_ind_om_minus]/4\n",
    "                cur_ind_om_minus += 1\n",
    "        else:\n",
    "            if i+1 < 2*L:\n",
    "                # there is a factor 1/2 before tunneling terms\n",
    "                mat[i,i+1] = omega_plus[cur_ind_om_plus]/4\n",
    "                cur_ind_om_plus += 1\n",
    "    return 1j*(mat-mat.T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TEST DIRECTION OF DOT\n",
    "M1 = np.array([1,0,0,0]).reshape((2,2))\n",
    "M2 = np.array([0,1,0,0]).reshape((2,2))\n",
    "np.dot(M1,M2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def combine_h_exps(_exp_h1,_exp_h2):\n",
    "    # THIS APPLIES FIRST exp_h1 and the exp_h2 so U = exp_h2 * exp_h1 * \\ket{\\psi}\n",
    "    return np.dot(_exp_h2,_exp_h1)\n",
    "def get_mat_maj_0(syssize,is_edge_minus=False):\n",
    "    mat_cor_majorana_res = np.zeros((2*syssize,2*syssize), dtype=np.complex128)\n",
    "    for i in range(2*syssize):\n",
    "        mat_cor_majorana_res[i,i] = 1\n",
    "        if i%2==0:\n",
    "            mat_cor_majorana_res[i,i+1] = 1j\n",
    "            mat_cor_majorana_res[i+1,i] = -1j\n",
    "            # if i is one of the edges - need to add minus sign\n",
    "            if is_edge_minus and (i == 0 or i == 2*syssize-2):\n",
    "                mat_cor_majorana_res[i,i+1] = -1*mat_cor_majorana_res[i,i+1]\n",
    "                mat_cor_majorana_res[i+1,i] = -1*mat_cor_majorana_res[i+1,i]\n",
    "    return mat_cor_majorana_res\n",
    "def get_zz_mat_maj_0(dist_x):\n",
    "    '''\n",
    "    The main difference in this function is that we start from i,j=1 (which is odd)\n",
    "    '''\n",
    "    mat_cor_majorana_res = np.zeros((2*dist_x,2*dist_x), dtype=np.complex128)\n",
    "    for i in range(2*dist_x):\n",
    "        mat_cor_majorana_res[i,i] = 1\n",
    "        # If i is odd, the correlation are even as there is +1 shift\n",
    "        if i%2==1:\n",
    "            mat_cor_majorana_res[i,i+1] = 1j\n",
    "            mat_cor_majorana_res[i+1,i] = -1j\n",
    "    return mat_cor_majorana_res\n",
    "def exp_cor_op(i1,i2,h_eff, T=None, mat_cor_majorana_0 = None, is_h_exp = False):\n",
    "    # TESTED - SEEMS TO WORK FINE. RETURNS REASONABLE OUTPUTS.\n",
    "    '''\n",
    "    i1 and i2 are indices that start at 0!\n",
    "    h_eff is the effective single particle Hamiltonian that evolves the state in the Majorana languag\n",
    "    such that we are calculating <\\psi|\\gamma_i1 \\gamma_i2|\\psi>, \n",
    "    where |\\psi> = U|0> = e^{-i H_eff}|0>, H_eff = \\sum \\gamma_i (h_eff)_{ij} \\gamma_j \n",
    "    and |0> is the zero fermionic state\n",
    "    '''\n",
    "    # This is the matrix that holds <0|gamma_i\\gamma_j|0> \n",
    "    if mat_cor_majorana_0 is None:\n",
    "        mat_cor_majorana_0 = get_mat_maj_0(L)\n",
    "    if h_eff is None:\n",
    "        # Returns the vacuum expectation value\n",
    "        return mat_cor_majorana_res[i1,i2]\n",
    "    \n",
    "    N = h_eff.shape[0] # This is the number of Majorana fermions = 2L\n",
    "    # First we calculate e^{i 4h_eff}, h_eff indeed satisfies h_eff = -h_eff.T\n",
    "    if T is None:\n",
    "        if is_h_exp:\n",
    "            T = matrix_power(h_eff,-1) # Should be e^{-i h_eff}^(-1) = expm(1j*4*A)\n",
    "        else:\n",
    "            T = expm(1j*4*h_eff) # expm(1j*4*A)\n",
    "    \n",
    "    #B1 = np.zeros((N,1))\n",
    "    #B2 = np.zeros((N,1))\n",
    "    # Initializing B defined in the notes. \n",
    "    # B is a 1D vector representing the operator O = \\sum_i B_i \\gamma_i, where \\gamma_i is the usual Majorana operator.\n",
    "    #B1[i1] = 1 \n",
    "    #B2[i2] = 1\n",
    "    \n",
    "    # According to the notes, we can add U^\\daggerU between the Majorana operators and then use formulas from the notes\n",
    "    #B1_res = T[:,i1]#T.dot(B1)\n",
    "    #B2_res = T[:,i2]#T.dot(B2)\n",
    "    \n",
    "    # mat_cor = M is the matrix that holds the coefficients for the vacuum averages \n",
    "    # such that M_{ij} <0|gamma_i\\gamma_j|0> needs to be added to the result\n",
    "    # TODO: optimize here the case that the calculation of mat_cor(i1,i2) for i1=i2 or i1=i2+1 or i1=i2-1, as all these cases result in 0\n",
    "    mat_cor = np.outer(T[:,i1].T,T[:,i2])\n",
    "    cor_res = np.sum(np.multiply(mat_cor,mat_cor_majorana_0))\n",
    "    \n",
    "    # return <\\psi | \\gamma_{i1} \\gamma_{i2} | \\psi>\n",
    "    return cor_res\n",
    "def get_cor_mat(subsystem_size, h_total, is_h_exp = False, is_edge_minus = False):\n",
    "    # create correlation matrix for h_total\n",
    "    M_cor = np.zeros((2*subsystem_size,2*subsystem_size), dtype=np.complex128)\n",
    "    if h_total is None:\n",
    "        M_cor = get_mat_maj_0(subsystem_size,False)\n",
    "        # Returns the free correlation anti symmetrized matrix\n",
    "        for i in range(2*subsystem_size):\n",
    "            M_cor[i,i] = 0\n",
    "        # Minus signs for the edges\n",
    "        if is_edge_minus:\n",
    "            M_cor[0,1] *= -1\n",
    "            M_cor[1,0] *= -1\n",
    "            if subsystem_size == L and L>1:\n",
    "                M_cor[2*L-2,2*L-1] *= -1\n",
    "                M_cor[2*L-1,2*L-2] *= -1\n",
    "        return M_cor\n",
    "    if is_h_exp:\n",
    "        precalc_T = matrix_power(h_total,-1)\n",
    "    else:\n",
    "        precalc_T = expm(1j*4*h_total)\n",
    "    precalc_mat_cor_majorana_0 = get_mat_maj_0(L,is_edge_minus)\n",
    "    # Iterate over rows\n",
    "    for i in range(2*subsystem_size):\n",
    "        # Iterate over columns\n",
    "        for j in range(2*subsystem_size):\n",
    "            if i==j or i>j:\n",
    "                # i=j is the diagonal, which is set to zero\n",
    "                # i>j are omitted as the matrix is anti-symmetric\n",
    "                continue\n",
    "            # this i is from factors of the parity\n",
    "            M_cor[i,j] = exp_cor_op(i,j,h_total, T= precalc_T, mat_cor_majorana_0 = precalc_mat_cor_majorana_0, is_h_exp=is_h_exp)\n",
    "            # This matrix is anti-symmetric\n",
    "            M_cor[j,i] = -1*M_cor[i,j]\n",
    "    # remove diagonal elements\n",
    "    for i in range(2*subsystem_size):\n",
    "        M_cor[i,i] = 0\n",
    "    return M_cor\n",
    "def get_zz_mat(start_site, x_dist, h_total, is_h_exp = False):\n",
    "    # create correlation matrix for h_total\n",
    "    M_cor = np.zeros((2*x_dist,2*x_dist), dtype=np.complex128)\n",
    "    if h_total is None:\n",
    "        M_cor = get_zz_mat_maj_0(x_dist,False)\n",
    "        # Returns the free correlation anti symmetrized matrix\n",
    "        for i in range(2*x_dist):\n",
    "            M_cor[i,i] = 0\n",
    "        return M_cor\n",
    "    if is_h_exp:\n",
    "        precalc_T = matrix_power(h_total,-1)\n",
    "    else:\n",
    "        precalc_T = expm(1j*4*h_total)\n",
    "    precalc_mat_cor_majorana_0 = get_mat_maj_0(L,False)\n",
    "    # Iterate over rows\n",
    "    for i in range(2*x_dist):\n",
    "        # Iterate over columns\n",
    "        for j in range(2*x_dist):\n",
    "            if i==j or i>j:\n",
    "                # i=j is the diagonal, which is set to zero\n",
    "                # i>j are omitted as the matrix is anti-symmetric\n",
    "                continue\n",
    "            # this i is from factors of the parity\n",
    "            M_cor[i,j] = exp_cor_op(i+2*start_site+1,j+2*start_site+1,h_total, T= precalc_T, mat_cor_majorana_0 = precalc_mat_cor_majorana_0, is_h_exp=is_h_exp)\n",
    "            # This matrix is anti-symmetric\n",
    "            M_cor[j,i] = -1*M_cor[i,j]\n",
    "    # remove diagonal elements\n",
    "    for i in range(2*x_dist):\n",
    "        M_cor[i,i] = 0\n",
    "    return M_cor\n",
    "def get_zz_cor_arr(start_site, max_dist, h_total, is_h_exp = False):\n",
    "    M = get_zz_mat(start_site,max_dist,h_total,is_h_exp)\n",
    "    zz_arr = []\n",
    "    for r in range(1,max_dist+1):\n",
    "        A = M[:2*(r),:2*(r)]\n",
    "        cur_cor = ((-1j)**(r))*pf.pfaffian(0.5*(A-A.T))\n",
    "        zz_arr.append(cur_cor)\n",
    "    return zz_arr\n",
    "def get_cor_single_exp(system_size,h_total_exp):\n",
    "    precalc_mat_cor_majorana_0 = get_mat_maj_0(system_size,False)\n",
    "    if h_total_exp is None:\n",
    "        # No Hamiltonian, we calculate the expected values on the simple two pairs\n",
    "        arr_single_pars = np.array([-1j*precalc_mat_cor_majorana_0[2*i,2*i+1] for i in range(system_size)])\n",
    "    else:\n",
    "        precalc_T = matrix_power(h_total_exp,-1)\n",
    "        # The -1j fixes the prefactors and results with the real <X_i>\n",
    "        arr_single_pars = np.array([-1j*exp_cor_op(2*i,2*i+1,h_total_exp, T= precalc_T, mat_cor_majorana_0 = precalc_mat_cor_majorana_0, is_h_exp=True) for i in range(system_size)])\n",
    "    # Return the expected parities over all the single qubits. Each quit is characterized by 2 Majorana fermions, \n",
    "    # where each of the Majorana fermions has an even index at the start. \n",
    "    # Thus, the pairs are consucutive. Returns the array arr, where arr[i] = <X_i>\n",
    "    return np.real(arr_single_pars)\n",
    "def get_cor_dw_exp(system_size,h_total_exp):\n",
    "    precalc_mat_cor_majorana_0 = get_mat_maj_0(system_size,False)\n",
    "    if h_total_exp is None:\n",
    "        # No Hamiltonian, we calculate the expected values on the simple two pairs\n",
    "        arr_dws = np.array([-1j*precalc_mat_cor_majorana_0[2*i+1,2*(i+1)] for i in range(system_size-1)])\n",
    "    else:\n",
    "        precalc_T = matrix_power(h_total_exp,-1)\n",
    "        # The -1j fixes the prefactors and results with the real <X_i>\n",
    "        arr_dws = np.array([-1j*exp_cor_op(2*i+1,2*(i+1),h_total_exp, T= precalc_T, mat_cor_majorana_0 = precalc_mat_cor_majorana_0, is_h_exp=True) for i in range(system_size-1)])\n",
    "    # Return the expected <Z_iZ_{i+1}> over all the pairs.\n",
    "    # Thus, the pairs are consucutive. Returns the array arr, where arr[i] = <Z_iZ_{i+1}>\n",
    "    return np.real(arr_dws)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adabatic sweeping functions and data saving"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Steps are the same definitions as in the draft\n",
    "# step_num=10\n",
    "# l = np.linspace(0,np.pi/2,step_num+2)[1:-1]\n",
    "# print(l)\n",
    "# l = [np.pi*(i/(2*(step_num+1))) for i in range(1,step_num+1)]\n",
    "# print(l)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_dws_using_exp_triv_to_ferro_end_only(cur_size,N_steps,debug_print=True,every_disorder_var_z=0,every_disorder_var_xx=0,static_disorder_var_z =0,static_disorder_var_xx = 0, calc_cors=False,rad=1,trot=False):\n",
    "    # This should be more efficient and exact compared with calc_ent_res\n",
    "    # TODO: test if this generates the same results as calc_ent_res\n",
    "    # TODO: FIX THIS GLOBAL USE OF L IN THE FUNCTIONS AND REMOVE IT\n",
    "    global L\n",
    "    L=cur_size\n",
    "    half_sys = int(L/2)\n",
    "    # UPDATE 27.03: Throw exception or Trotterization\n",
    "    if trot:\n",
    "        raise Exception(\"Check exact definitions for Trotterization and N_steps\")\n",
    "        steps_list = np.array(list(range(1,N_steps+1)))\n",
    "        dt =0.1\n",
    "        curT = 0.5*dt*N_steps\n",
    "    else:\n",
    "        # UPDATE 27.03: We remove the first and last steps\n",
    "        # N_steps -> N_steps+2 as we want the old partitions, but we remove the first and last step\n",
    "        steps_list = np.linspace(0,np.pi/2,N_steps+2)[1:-1]\n",
    "\n",
    "    # Though the array doesn't contain complex numbers, we refer to them as complex for simple multiplication with complex numbers\n",
    "    total_dw_end = 0\n",
    "    # TODO: Parametrize this variable\n",
    "    max_dist = int(L/5)\n",
    "    zz_cors_end = np.zeros(shape=(max_dist,),dtype=np.float64)\n",
    "    \n",
    "    # Disorder parameters\n",
    "    is_disorder = False\n",
    "    if every_disorder_var_z is not None or every_disorder_var_xx is not None:\n",
    "        is_disorder = True\n",
    "        disorder_every_step = True\n",
    "    # Static disorder\n",
    "    if static_disorder_var_z is not None or static_disorder_var_xx is not None:\n",
    "        is_disorder = True\n",
    "        static_disorder = True\n",
    "        stat_disorder_arr_mu =np.random.normal(0, static_disorder_var_z, cur_size)\n",
    "        stat_disorder_arr_t = np.random.normal(0, static_disorder_var_xx, cur_size-1)\n",
    "    # This is for the plus state\n",
    "    cur_step_num = 0\n",
    "    cur_h_exp = None\n",
    "    r0 = rad\n",
    "    debug_step_skip = int(N_steps/10)\n",
    "\n",
    "    # Going over the path adiabatically\n",
    "    for step in steps_list:\n",
    "        if debug_print and cur_step_num % debug_step_skip == 0:\n",
    "            print(cur_step_num)\n",
    "        # Calculate the next step parameters\n",
    "        if trot: # The linear trotterized path\n",
    "            curg = -1*(2-(step*dt)/curT)\n",
    "            curJ = -1*(step*dt)/curT\n",
    "            #s = float(cur_step_num)/float(N_steps)\n",
    "            alpha_cur = curg*dt # H_{PM} coefficient\n",
    "            beta_cur =  curJ*dt # H_{FM} coefficient\n",
    "        else:\n",
    "            theta = step\n",
    "            # The half here seems to be artifact of some notation\n",
    "            alpha_cur = 2*0.5*r0 * np.cos(theta) # H_{PM} coefficient\n",
    "            beta_cur = 2*0.5*r0*np.sin(theta) # H_{FM} coefficient\n",
    "            # UPDATE 13.09 changed r_0 to be 1\n",
    "        if is_disorder:\n",
    "            disorder_arr_mu = np.zeros(cur_size)\n",
    "            disorder_arr_t = np.zeros(cur_size-1)\n",
    "            if static_disorder:\n",
    "                disorder_arr_mu = disorder_arr_mu + stat_disorder_arr_mu\n",
    "                disorder_arr_t = disorder_arr_t + stat_disorder_arr_t\n",
    "            if disorder_every_step:\n",
    "                disorder_arr_mu = disorder_arr_mu + np.random.normal(0, every_disorder_var_z, cur_size)\n",
    "                disorder_arr_t = disorder_arr_t + np.random.normal(0, every_disorder_var_xx, cur_size-1) # There are only cur_size-1 pairs\n",
    "            cur_h_X = h_mat_disorder(cur_size,disorder_arr_mu+alpha_cur,np.zeros(cur_size-1))\n",
    "            cur_h_ZZ = h_mat_disorder(cur_size,np.zeros(cur_size),disorder_arr_t+beta_cur)              \n",
    "        else:\n",
    "            cur_h_X = h_mat(cur_size,alpha_cur,0)\n",
    "            cur_h_ZZ = h_mat(cur_size,0,beta_cur)\n",
    "        h_ZZ_exp = expm(-1*1j*4*cur_h_ZZ)\n",
    "        h_X_exp = expm(-1*1j*4*cur_h_X)\n",
    "        # start of the loop intializes cur_h\n",
    "        if cur_h_exp is None:\n",
    "            # EDIT: CORRECT THIS, START FROM H_FM at EACH STEP as we start from the |+> state\n",
    "            # First we apply the ZZ interaction H_{FM} with coefficient beta and only then we apply X of H_{PM} wth parameter alpha\n",
    "            cur_h_exp = combine_h_exps(h_ZZ_exp,h_X_exp) # this is F(\\alpha,\\beta)\n",
    "        else:\n",
    "            # First we apply the ZZ interaction H_{FM} with coefficient beta and only then we apply X of H_{PM} wth parameter alpha\n",
    "            cur_h_exp = combine_h_exps(cur_h_exp,combine_h_exps(h_ZZ_exp,h_X_exp)) # this is F(\\alpha,\\beta)\n",
    "        cur_step_num += 1\n",
    "    # Calculate the number of domain walls at the end\n",
    "    arr_dws = get_cor_dw_exp(cur_size,cur_h_exp)\n",
    "    # TODO: Choose L or L-1, currently we choose L-1\n",
    "    total_dw_end = 0.5*(cur_size-1-sum(arr_dws))\n",
    "    #total_dw_end = 0.5*(cur_size-sum(arr_dws))\n",
    "    # Calculate the correlation array at the end\n",
    "    if calc_cors:  \n",
    "        zz_cors_end = np.real(get_zz_cor_arr(half_sys,max_dist,cur_h_exp,is_h_exp=True))\n",
    "    else:\n",
    "        zz_cors_end = None\n",
    "    return total_dw_end,zz_cors_end"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Repeat the calculation above with more optimized version that can run: Do the calculation in optimized version of 10000 steps adiabaticity for different system sizes.\n",
    "# ONLY XX DISORDER\n",
    "import itertools\n",
    "def save_disorder_data(cur_L_size_arr,cur_N_opt,cur_disorder_xx_var_arr=None,cur_static_disorder_xx_var_arr=None,cur_disorder_count=2,_model_name='highdisorder',radiu=1,to_trotter=False,setNum=-1):\n",
    "    if cur_static_disorder_xx_var_arr is None:\n",
    "        cur_static_disorder_xx_var_arr = [0]\n",
    "    if cur_disorder_xx_var_arr is None:\n",
    "        cur_disorder_xx_var_arr = [0]\n",
    "    cur_N_max = cur_N_opt[-1]\n",
    "    for L_size in cur_L_size_arr:\n",
    "        dw_sat_end_dict = dict()\n",
    "        dw_sat_end_std_dict = dict()\n",
    "        name = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}_endstep_new_nsteps_definition_corrected_noise'.format(str(L_size),str(cur_N_max),str(radiu),str(to_trotter),_model_name)\n",
    "        for cur_disorder_var_x in cur_disorder_xx_var_arr:\n",
    "            if cur_disorder_var_x is None:\n",
    "                cur_disorder_var_x = 0\n",
    "            for cur_stat_disorder_var_x in cur_static_disorder_xx_var_arr:\n",
    "                dw_arr_end = dict()\n",
    "                dw_arr_end_std = dict()\n",
    "                dw_arr_full_data = dict()\n",
    "                for cur_n_opt in cur_N_opt:\n",
    "                    cur_disorder_arr_res = []\n",
    "                    #cur_zz_cors_arr = []\n",
    "                    for cur_dis_ind in range(cur_disorder_count):\n",
    "                        print(cur_dis_ind)\n",
    "                        total_dws_val,_ = calc_dws_using_exp_triv_to_ferro_end_only(L_size,int(cur_n_opt),debug_print=False,static_disorder_var_xx=cur_stat_disorder_var_x,static_disorder_var_z=cur_stat_disorder_var_x,every_disorder_var_z=cur_disorder_var_x,every_disorder_var_xx=cur_disorder_var_x,rad=radiu,trot=to_trotter)\n",
    "                        # TODO: choose L or L-1, currently we choose L-1\n",
    "                        cur_disorder_arr_res.append(total_dws_val/(L_size-1))\n",
    "                        # Currently try L\n",
    "                    print(cur_n_opt)\n",
    "                    #cor_len_per_point_arrs =  obtain_cor_arr(cur_zz_cors_arr)\n",
    "                    dw_arr_full_data[cur_n_opt] = np.array(cur_disorder_arr_res)\n",
    "                    dw_arr_end[cur_n_opt] = np.mean(np.array(cur_disorder_arr_res))\n",
    "                    if setNum == -1:\n",
    "                        dw_arr_end_std[cur_n_opt] = np.std(np.array(cur_disorder_arr_res))\n",
    "                    else:\n",
    "                        splited_arr = np.array_split(np.array(cur_disorder_arr_res),setNum)\n",
    "                        avg_arr = np.array([np.mean(splited_arr[i]) for i in range(setNum)])\n",
    "                        dw_arr_end_std[cur_n_opt] = np.std(avg_arr)\n",
    "                print(dw_arr_end)\n",
    "                print(dw_arr_end_std)\n",
    "                dw_sat_end_dict[(cur_disorder_var_x,cur_stat_disorder_var_x)] = dw_arr_end\n",
    "                dw_sat_end_std_dict[(cur_disorder_var_x,cur_stat_disorder_var_x)] = dw_arr_end_std\n",
    "        # DEBUG: Save full data with all counts\n",
    "        # np.save(name+'_full_data_arr.npy',dw_arr_full_data)\n",
    "        np.save(name+'_data_mean_dict.npy',dw_sat_end_dict)\n",
    "        np.save(name+'_data_std_dict.npy',dw_sat_end_std_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot function definitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_LEGEND = 22\n",
    "FONTSIZE_AXIS = 24\n",
    "FONTSIZE_TICKS = 20\n",
    "FONTSIZE_TITLE = 16\n",
    "\n",
    "def plot_kz_scaling_disorder(xaxis,sys_size,max_N,model_name,rad=1,trot=False):\n",
    "    # Update 27.03.22: Remove trot variable usage and add to the filenames the new definiton\n",
    "    # Also, only output pdf files\n",
    "    # Load data from files\n",
    "    if trot:\n",
    "        raise Exception(\"Check exact definitions for Trotterization and N_steps\")\n",
    "    try:\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(max_N),str(rad),str(trot),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_std_dict.npy', allow_pickle=True).item()\n",
    "    except (FileNotFoundError):\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_std_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "    # Choose the xaxis to be the N's of the first in the dictionary\n",
    "    if xaxis is None:\n",
    "        xaxis = list(list(yaxis_dict.values())[0].keys())\n",
    "        print(xaxis)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_xticks(xaxis) # NO MINOR BOOL FOR NEWER MATPLOTLIB\n",
    "    # TODO: We divide N_{ex} by L-1, change to L in the case of periodic boundary conditions\n",
    "    ax.set_ylabel(r'Excitations density $\\frac{N_{\\rm ex}}{L}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$N_{\\rm steps}$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for k,yaxis in sorted(yaxis_dict.items()):\n",
    "        yerr_arr = yaxis_std_dict[k]\n",
    "        l_yaxis = []\n",
    "        l_yerr_arr = []\n",
    "        for n_opt in xaxis:\n",
    "            l_yaxis.append(yaxis[n_opt])\n",
    "            l_yerr_arr.append(yerr_arr[n_opt])\n",
    "        print(l_yaxis)\n",
    "        ax.errorbar(xaxis,l_yaxis,ls='--',label=r'$\\sigma='+str(k)+r'$',marker='o',markersize=4,capsize=2,yerr=l_yerr_arr)\n",
    "        fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(np.array(xaxis[:])[:]), np.log(np.array(l_yaxis[:])[:]))\n",
    "        fit_slope = fit_params[0][1]\n",
    "        #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "        print('sigma={0} with slope={1}'.format(str(k), str(fit_slope)))\n",
    "    # TODO: Unique names\n",
    "    name = 'kz_scaling_'+str(sys_size)+'system_fermion_mapping_disorder_everystep=True_optimized_new_version'\n",
    "\n",
    "    ax.legend(loc='lower left',fontsize=FONTSIZE_LEGEND)\n",
    "    #ax.figure.savefig(name+'.png')\n",
    "    #ax.figure.savefig(name+'.svg')\n",
    "    ax.figure.savefig(name+'.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_LEGEND = 12\n",
    "FONTSIZE_AXIS = 24\n",
    "FONTSIZE_TICKS = 20\n",
    "FONTSIZE_TITLE = 16\n",
    "\n",
    "def plot_kz_scaling_disorder_static_new_format(xaxis,sys_size,max_N,model_name,rad=1,trot=False):\n",
    "    # Update 27.03.22: Remove trot variable usage and add to the filenames the new definiton\n",
    "    # Also, only output pdf files\n",
    "    # Load data from files\n",
    "    if trot:\n",
    "        raise Exception(\"Check exact definitions for Trotterization and N_steps\")\n",
    "    try:\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(max_N),str(rad),str(trot),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_std_dict.npy', allow_pickle=True).item()\n",
    "    except (FileNotFoundError):\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_data_std_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "    # Choose the xaxis to be the N's of the first in the dictionary\n",
    "    if xaxis is None:\n",
    "        xaxis = list(list(yaxis_dict.values())[0].keys())\n",
    "        print(xaxis)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_xticks(xaxis) # NO MINOR BOOL FOR NEWER MATPLOTLIB\n",
    "    # TODO: We divide N_{ex} by L-1, change to L in the case of periodic boundary conditions\n",
    "    ax.set_ylabel(r'Excitations density $\\frac{N_{\\rm ex}}{L}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$N_{\\rm steps}$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for k,yaxis in sorted(yaxis_dict.items()):\n",
    "        every_disorder = k[0]\n",
    "        static_disorder = k[1]\n",
    "        yerr_arr = yaxis_std_dict[k]\n",
    "        l_yaxis = []\n",
    "        l_yerr_arr = []\n",
    "        for n_opt in xaxis:\n",
    "            l_yaxis.append(yaxis[n_opt])\n",
    "            l_yerr_arr.append(yerr_arr[n_opt])\n",
    "        print(l_yaxis)\n",
    "        ax.errorbar(xaxis,l_yaxis,ls='--',label=r'$\\sigma_{\\rm step}='+str(every_disorder)+r', \\sigma_{\\rm static}='+str(static_disorder)+r' $',marker='o',markersize=4,capsize=2,yerr=l_yerr_arr)\n",
    "        fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(np.array(xaxis[:])[:]), np.log(np.array(l_yaxis[:])[:]))\n",
    "        fit_slope = fit_params[0][1]\n",
    "        #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "        print('sigma={0} with slope={1}'.format(str(k), str(fit_slope)))\n",
    "    # TODO: Unique names\n",
    "    name = 'plot_'+load_filename#'kz_scaling_'+str(sys_size)+'system_fermion_mapping_disorder_everystep=True_optimized_new_version_new_format_with_static_disorder_labels'\n",
    "\n",
    "    ax.legend(loc='lower left',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(name+'.pdf')\n",
    "    #ax.figure.savefig(name+'.svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Disorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static disorder with constant every step disorder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [3e-2]\n",
    "static_disorder_xx_var_arr = [1e-3,1e-2,3e-2,5e-2,1e-1,3e-1,5e-1]\n",
    "max_N = 100#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "L_sizes = [50]\n",
    "disorder_count = 10\n",
    "is_disorder_every_step = False\n",
    "model_name = 'static_disorder_with_everystep=0.03'\n",
    "print(N_opt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "for cur_sys_size in L_sizes:\n",
    "    # TODO: save them with actual names\n",
    "    plot_kz_scaling_disorder_static_new_format(N_opt,cur_sys_size,N_opt[-1],model_name,rad=radi,trot=to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every step disorder with constant static disorder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [1e-3,1e-2,3e-2,5e-2,1e-1,3e-1,5e-1]\n",
    "static_disorder_xx_var_arr = [3e-1]\n",
    "max_N = 100#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "L_sizes = [50]\n",
    "disorder_count = 30\n",
    "model_name = 'everystep_disorder_with_static='+str(static_disorder_xx_var_arr[0])\n",
    "print(N_opt)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "for cur_sys_size in L_sizes:\n",
    "    # TODO: save them with actual names\n",
    "    plot_kz_scaling_disorder_static_new_format(N_opt,cur_sys_size,N_opt[-1],model_name,rad=radi,trot=to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General structure for obtaining data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [1e-2]#[1e-2,1e-2,3e-2,5e-2,1e-1,3e-1,5e-1,1]\n",
    "static_disorder_xx_var_arr = [0,5e-3,1e-2,5e-2,1e-1]\n",
    "max_N = 600#30\n",
    "N_start = 20\n",
    "radi = 1 # IGNORE THIS CURRENTLY\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,25))\n",
    "L_sizes = [40]\n",
    "disorder_count = 6\n",
    "is_disorder_every_step = True\n",
    "model_name = 'static_and_every_step_disorder'\n",
    "print(N_opt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# No disorder\n",
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "for cur_sys_size in L_sizes:\n",
    "    plot_kz_scaling_disorder_static_new_format(N_opt,cur_sys_size,N_opt[-1],model_name,rad=radi,trot=to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d = {100: 0.06758624097594657, 200: 0.04675348106663357, 300: 0.03752408889092279, 400: 0.03201145820373713, 500: 0.02823126496497874, 600: 0.0254294377277694, 700: 0.023255201721835594, 800: 0.021503561198011445, 900: 0.020055099076554228, 1000: 0.018828253510122017, 1100: 0.017770106962242294, 1200: 0.016847583010850375, 1300: 0.016033498956973702, 1400: 0.015309698768704472, 1500: 0.014659320928273045, 1600: 0.014069913264894596, 1700: 0.013533342072178656, 1800: 0.013041787013080619, 1900: 0.012590369835634385, 2000: 0.01217325156981508, 2100: 0.011785970754115582, 2200: 0.011425326924183521, 2300: 0.011088104386267617, 2400: 0.010772678202906394, 2500: 0.010476381873587293}\n",
    "for k in d:\n",
    "    d[k] = d[k] + (1/(2*128))\n",
    "print(d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verfication data for $L=4,5$ without disorder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]#[1e-2,1e-2,3e-2,5e-2,1e-1,3e-1,5e-1,1]\n",
    "max_N = 30#300\n",
    "N_start = 1\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "L_sizes = [4]\n",
    "disorder_count = 1\n",
    "is_disorder_every_step = True\n",
    "to_trotter_radius = False\n",
    "N_opt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the data\n",
    "model_name = 'no_disorder_verfication_test_to_delete'\n",
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "for cur_sys_size in L_sizes:\n",
    "    plot_kz_scaling_disorder([int(i) for i in N_opt[5:]],cur_sys_size,N_opt[-1],model_name,rad=1,trot=to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(abs(np.array(means[100:])-np.array(free_fermion_dim_res[100:])))/(np.array(means[100:]))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "means"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "free_fermion_dim_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.array(devs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d={2: 0.3078018659785016, 3: 0.28028028060625443, 4: 0.2508101277413441, 5: 0.22123924788454963, 6: 0.19331861835893638, 7: 0.1682735798460719, 8: 0.1466935826662156, 9: 0.12859618729101543, 10: 0.11359565087588182, 11: 0.10110974720731943, 12: 0.09054531563144312, 13: 0.081421713982558, 14: 0.07341727262636095, 15: 0.0663490536208479, 16: 0.060113521915755884, 17: 0.054621200385468395, 18: 0.04975228387582017, 19: 0.04534644001622312, 20: 0.0412244453829231, 21: 0.03722745348943753, 22: 0.03325503597973493, 23: 0.029286161840575458, 24: 0.02537571246591212, 25: 0.021629119187694463, 26: 0.018165394293336263, 27: 0.01508172380136715, 28: 0.0124305102150708, 29: 0.010213863952553007, 30: 0.008393732216180227, 31: 0.006910785165166559, 32: 0.005703488113602255, 33: 0.004720598890155118, 34: 0.003924400215581307, 35: 0.0032863574508054474, 36: 0.0027797767710430557, 37: 0.0023744657147835024, 38: 0.0020365341001233883, 39: 0.0017334168238287335, 40: 0.0014414278393460138, 41: 0.0011518705193151273, 42: 0.0008723874499081777, 43: 0.0006224115742550218, 44: 0.0004242332884846582, 45: 0.00029313992891329593, 46: 0.0002305022934334655, 47: 0.00022245490839482388, 48: 0.00024452021340008273, 49: 0.00027015763522656816, 50: 0.0002797535882321789, 51: 0.00026656283743046555, 52: 0.00023749398135889757, 53: 0.00020876950984168494, 54: 0.00019848194471131878, 55: 0.00021912470051040542, 56: 0.00027294041634425464, 57: 0.00035158827662902975, 58: 0.00043980701138875605, 59: 0.0005212146476432089, 60: 0.0005837372137296551, 61: 0.000622586164376826, 62: 0.0006399410645906434, 63: 0.0006419588477756625, 64: 0.0006347921042892946, 65: 0.0006215413387499572, 66: 0.0006014508078297324, 67: 0.0005715127631717998, 68: 0.0005295041648532747, 69: 0.0004768426788450686, 70: 0.00041976950646623507, 71: 0.00036815924452075305, 72: 0.0003323488776436623, 73: 0.00031928275004222684, 74: 0.0003296005018182206, 75: 0.0003569227535020018, 76: 0.00038969022340962223, 77: 0.00041487797254935455, 78: 0.0004221720839111172, 79: 0.00040705409965328493, 80: 0.00037172372788655633, 81: 0.0003236766112632387, 82: 0.00027266092145324566, 83: 0.0002272953848820869, 84: 0.00019263757328791264, 85: 0.00016948089620509124, 86: 0.00015537952787640177, 87: 0.0001466960712882178, 88: 0.0001406263600859011, 89: 0.0001362953578063116, 90: 0.00013454116547813774, 91: 0.00013665522359262994, 92: 0.0001428299858489135, 93: 0.00015117427304785677, 94: 0.00015785955747005254, 95: 0.00015840664679474722, 96: 0.00014956895894317293, 97: 0.00013096499658682426, 98: 0.0001056912943679933, 99: 7.956973556858908e-05, 100: 5.9262033591892895e-05, 101: 4.9967460022874256e-05, 102: 5.3601646356519574e-05, 103: 6.816157510799077e-05, 104: 8.850226186241095e-05, 105: 0.00010819027869316915, 106: 0.00012169200528303985, 107: 0.00012606220017641334, 108: 0.00012154554787306242, 109: 0.00011097225960191952, 110: 9.831379018347224e-05, 111: 8.706478346535522e-05, 112: 7.911921348724427e-05, 113: 7.453112068163865e-05, 114: 7.212981036526973e-05, 115: 7.058601553019035e-05, 116: 6.935888451176098e-05, 117: 6.905520566384087e-05, 118: 7.104302479817488e-05, 119: 7.653295380305458e-05, 120: 8.560195483275912e-05, 121: 9.66702730184436e-05, 122: 0.00010673998378470569, 123: 0.00011235696992909865, 124: 0.00011092319130053448, 125: 0.00010181174149611245, 126: 8.68001584158525e-05, 127: 6.961035595139571e-05, 128: 5.4708568479037524e-05, 129: 4.582011943595384e-05, 130: 4.472553078708567e-05, 131: 5.078318158879824e-05, 132: 6.132529693251658e-05, 133: 7.272718556235643e-05, 134: 8.169914133537848e-05, 135: 8.629540522390548e-05, 136: 8.628818741692375e-05, 137: 8.28418237673878e-05, 138: 7.771471077266945e-05, 139: 7.239066396480531e-05, 140: 6.752957022282402e-05, 141: 6.294582337051935e-05, 142: 5.8061148016319564e-05, 143: 5.255643489462022e-05, 144: 4.6863061717676104e-05, 145: 4.22211072997151e-05, 146: 4.024581506778535e-05, 147: 4.21848043854108e-05, 148: 4.820672550650181e-05, 149: 5.706571413588435e-05, 150: 6.633299057183055e-05, 151: 7.314447847521037e-05, 152: 7.518755678478002e-05, 153: 7.153930624485365e-05, 154: 6.302072722730312e-05, 155: 5.1923826633733704e-05, 156: 4.121901246216068e-05, 157: 3.35533299761955e-05, 158: 3.0422296413775325e-05, 159: 3.181314531593612e-05, 160: 3.6415693752900324e-05, 161: 4.226698855717507e-05, 162: 4.7534262607099954e-05, 163: 5.111160383508562e-05, 164: 5.281419108145909e-05, 165: 5.3146104317502996e-05, 166: 5.280613889594813e-05, 167: 5.2198815504223006e-05, 168: 5.119385488749906e-05, 169: 4.924315637892344e-05, 170: 4.578451469276305e-05, 171: 4.0719074029841686e-05, 172: 3.4709169872434074e-05, 173: 2.912215980854782e-05, 174: 2.5607756551434175e-05, 175: 2.5468078877969685e-05, 176: 2.908394451184293e-05, 177: 3.565070222710798e-05, 178: 4.335545800689585e-05, 179: 4.9946768951203424e-05, 180: 5.3485184325722614e-05, 181: 5.298761032449898e-05, 182: 4.872164227250527e-05, 183: 4.204824023634757e-05, 184: 3.489134861082525e-05, 185: 2.905552830967384e-05, 186: 2.5659549223585227e-05, 187: 2.4889412897947476e-05, 188: 2.613023435856417e-05, 189: 2.8377105365295918e-05, 190: 3.071869024588262e-05, 191: 3.26765341193506e-05, 192: 3.426748604717922e-05, 193: 3.579443163156023e-05, 194: 3.74980617620461e-05, 195: 3.926281349890074e-05, 196: 4.0538164362224904e-05, 197: 4.0528881994905674e-05, 198: 3.857501430652258e-05, 199: 3.454539457548034e-05, 200: 2.9051657825740484e-05, 201: 2.3361502081487018e-05, 202: 1.901897967963399e-05, 203: 1.7309333925112174e-05, 204: 1.8779279249314662e-05, 205: 2.3008181104492802e-05, 206: 2.8727365176006714e-05, 207: 3.424540483150847e-05, 208: 3.8015619459893145e-05, 209: 3.912885953893941e-05, 210: 3.755085838733999e-05, 211: 3.403173068032098e-05, 212: 2.9748343795839755e-05, 213: 2.5841930116750806e-05, 214: 2.3042297916312886e-05, 215: 2.151754940589908e-05, 216: 2.098067261313563e-05, 217: 2.0970738707326575e-05, 218: 2.1156418598529864e-05, 219: 2.1511894677657868e-05, 220: 2.2285850959979925e-05, 221: 2.378878684953604e-05, 222: 2.6113878666578216e-05, 223: 2.8941051302429816e-05, 224: 3.15375073253558e-05, 225: 3.2977456120815894e-05, 226: 3.250020785268889e-05, 227: 2.985598666567964e-05, 228: 2.548464733161578e-05, 229: 2.0436851775847604e-05, 230: 1.6052941727240366e-05, 231: 1.3516388331173227e-05, 232: 1.3452679774298796e-05, 233: 1.5727791935852647e-05, 234: 1.952029750285078e-05, 235: 2.3631898838287018e-05, 236: 2.6908760366151085e-05, 237: 2.8608475617734424e-05, 238: 2.857873922645915e-05, 239: 2.719822815470489e-05, 240: 2.5130188534191095e-05, 241: 2.3011819994348908e-05, 242: 2.121811531770786e-05, 243: 1.9793437870285757e-05, 244: 1.8560533975087168e-05, 245: 1.733355277762823e-05, 246: 1.611618828497215e-05, 247: 1.5177641874937938e-05, 248: 1.4961102414057237e-05, 249: 1.5862666011649356e-05, 250: 1.7984667147900406e-05, 251: 2.0985872422755552e-05, 252: 2.4112978426305e-05, 253: 2.6419749870202718e-05, 254: 2.709627614749799e-05, 255: 2.577829957032633e-05, 256: 2.2708897689997325e-05, 257: 1.8681684962107425e-05, 258: 1.4782223508628492e-05, 259: 1.202545096031423e-05, 260: 1.1027533833714761e-05, 261: 1.183409130941134e-05, 262: 1.396108108719929e-05, 263: 1.661832821768054e-05, 264: 1.901530242683543e-05, 265: 2.0623135719969692e-05, 266: 2.1294922734726995e-05, 267: 2.1213749258860066e-05, 268: 2.0713940900307648e-05, 269: 2.0072376766563e-05, 270: 1.9371984525988022e-05, 271: 1.849831310100451e-05, 272: 1.726257180948802e-05, 273: 1.5582145288036326e-05, 274: 1.3620805648928425e-05, 275: 1.1808543832462584e-05, 276: 1.0716918079856086e-05, 277: 1.0834610609133932e-05, 278: 1.2338251099253128e-05, 279: 1.496239339741449e-05, 280: 1.8035069620614408e-05, 281: 2.067728841422441e-05, 282: 2.2095427625619852e-05, 283: 2.1854677301128927e-05, 284: 2.0027198124143258e-05, 285: 1.7158251272464398e-05, 286: 1.406619155641664e-05, 287: 1.1557266210271422e-05, 288: 1.0166648019412547e-05, 289: 1.0021219759940673e-05, 290: 1.0865403524462636e-05, 291: 1.222291217854258e-05, 292: 1.3614110970043095e-05, 293: 1.4732644075884002e-05, 294: 1.5511271764134804e-05, 295: 1.606210567284272e-05, 296: 1.6535037909326533e-05, 297: 1.6973872698685295e-05, 298: 1.7247131005852328e-05, 299: 1.709182556930422e-05, 300: 1.6252026445099748e-05}\n",
    "free_fermion_dim_res= []\n",
    "for k in range(2,301):\n",
    "    free_fermion_dim_res.append(d[k])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "d[163]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NISQ devices plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [1e-3,5e-3,1e-2,5e-2,1e-1,3e-1]#[1e-2,1e-2,3e-2,5e-2,1e-1,3e-1,5e-1,1]\n",
    "max_N = 1000#30\n",
    "N_start = 2\n",
    "count_points = 80\n",
    "N_opt = list(range(2,30,2))+list(np.linspace(N_start,max_N,count_points).astype(int))\n",
    "L_sizes = [50]\n",
    "disorder_count = 20\n",
    "is_disorder_every_step = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the data\n",
    "model_name = 'nisq_parameters_low_nsteps'\n",
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,disorder_count,model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "model_name = 'merged_nisq_parameters'\n",
    "for cur_sys_size in L_sizes:\n",
    "    plot_kz_scaling_disorder(None,cur_sys_size,N_opt[-1],model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check scalings of static disorder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Copy scaling code from the other file to here\n",
    "# Plot the saturation values as a function of the disorder\n",
    "def plot_sat_values(sys_size,max_N,rad=1,trot=False):\n",
    "    # Load data from files\n",
    "    try:\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(max_N),str(rad),str(trot),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "    except (FileNotFoundError):\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$\\sigma$ disorder',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "    xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "    yaxis = []\n",
    "    yerr_arr = []\n",
    "\n",
    "    # Get the every step noise\n",
    "    k_every_step_val = sorted(yaxis_dict.keys())[0][0]\n",
    "\n",
    "    for k in sorted(yaxis_dict.keys()):\n",
    "        if k == (k_every_step_val,0):\n",
    "            continue\n",
    "        #yerr_arr = yaxis_std_dict[k]\n",
    "        #l_yaxis = []\n",
    "        #l_yerr_arr = []\n",
    "        #for n_opt in xaxis:\n",
    "        #    l_yaxis.append(yaxis[n_opt])\n",
    "        #    l_yerr_arr.append(yerr_arr[n_opt])\n",
    "\n",
    "        max_N_to_plot = max(yaxis_dict[k].keys())\n",
    "        #print(yaxis_dict[k])\n",
    "        \n",
    "        # Ignore negative values\n",
    "        val_y = yaxis_dict[k][max_N_to_plot]-yaxis_dict[(k_every_step_val,0)][max_N_to_plot]\n",
    "        if val_y <0:\n",
    "            continue\n",
    "\n",
    "        xaxis.append(k[1])\n",
    "        yaxis.append(yaxis_dict[k][max_N_to_plot]-yaxis_dict[(k_every_step_val,0)][max_N_to_plot])\n",
    "        yerr_arr.append(yaxis_std_dict[k][max_N_to_plot])\n",
    "    print(xaxis)\n",
    "    print(yaxis)\n",
    "    print(yerr_arr)\n",
    "    ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=4,capsize=2,yerr=yerr_arr,label=r'$L='+str(sys_size)+'$')\n",
    "    fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(np.array(xaxis)), np.log(np.array(yaxis)))\n",
    "    print(fit_params[0])\n",
    "    fit_slope = fit_params[0][1]\n",
    "    #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "    print('L={0}= {1}'.format(str(sys_size),str(fit_slope)))\n",
    "    ax.plot(np.array(xaxis),np.exp(fit_params[0][0]+fit_params[0][1]*np.log(np.array(xaxis))),color='tab:red',ls='-',label=r'Linear fit, $f(\\sigma)='+str(fit_slope)+r'\\sigma+'+str(fit_params[0][0])+r'$')\n",
    "    #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "    #print('sigma={0} with slope={1}'.format(str(k), str(fit_slope)))\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=15)\n",
    "    ax.figure.savefig(load_filename+'.png',dpi=300)\n",
    "    ax.figure.savefig(load_filename+'.svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Copy scaling code from the other file to here\n",
    "# # Plot the saturation values as a function of the disorder\n",
    "# def plot_sat_values_with_fixed_slope(sys_size,max_N,rad=1,trot=False,match_guess=False):\n",
    "#     # Load data from files\n",
    "#     try:\n",
    "#         load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(max_N),str(rad),str(trot),model_name)\n",
    "#         yaxis_dict = np.load(load_filename+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "#         yaxis_std_dict = np.load(load_filename+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "#     except (FileNotFoundError):\n",
    "#         load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "#         yaxis_dict = np.load(load_filename+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "#         yaxis_std_dict = np.load(load_filename+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "#     ax.set_xlabel(r'$\\sigma$ disorder',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "#     # Set log scale\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_xscale('log')  \n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "#     xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "#     yaxis = []\n",
    "#     yerr_arr = []\n",
    "\n",
    "#     # Get the every step noise\n",
    "#     k_every_step_val = sorted(yaxis_dict.keys())[0][0]\n",
    "\n",
    "#     for k in sorted(yaxis_dict.keys()):\n",
    "#         if k == (k_every_step_val,0):\n",
    "#             continue\n",
    "#         #yerr_arr = yaxis_std_dict[k]\n",
    "#         #l_yaxis = []\n",
    "#         #l_yerr_arr = []\n",
    "#         #for n_opt in xaxis:\n",
    "#         #    l_yaxis.append(yaxis[n_opt])\n",
    "#         #    l_yerr_arr.append(yerr_arr[n_opt])\n",
    "\n",
    "#         max_N_to_plot = max(yaxis_dict[k].keys())\n",
    "#         #print(yaxis_dict[k])\n",
    "        \n",
    "#         # Ignore negative values\n",
    "#         val_y = yaxis_dict[k][max_N_to_plot]-yaxis_dict[(k_every_step_val,0)][max_N_to_plot]\n",
    "#         if val_y <0:\n",
    "#             continue\n",
    "\n",
    "#         xaxis.append(k[1])\n",
    "#         yaxis.append(yaxis_dict[k][max_N_to_plot]-yaxis_dict[(k_every_step_val,0)][max_N_to_plot])\n",
    "#         yerr_arr.append(yaxis_std_dict[k][max_N_to_plot])\n",
    "#     print(xaxis)\n",
    "#     print(yaxis)\n",
    "#     print(yerr_arr)\n",
    "#     ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=4,capsize=2,yerr=yerr_arr,label=r'$L='+str(sys_size)+'$')\n",
    "    \n",
    "#     if match_guess:\n",
    "#         guess_co = np.log(0.36)\n",
    "#         ax.plot(np.array(xaxis),np.exp(guess_co+2*np.log(np.array(xaxis))),color='tab:red',ls='-',label=r'Linear fit, $f(\\sigma)='+str(2)+r'\\sigma+'+str(guess_co)+r'$')\n",
    "#         load_filename += '_guess_coefficient=0.36'\n",
    "#     else:\n",
    "#         fit_params = scipy.optimize.curve_fit(lambda x,a: a+2*x, np.log(np.array(xaxis)), np.log(np.array(yaxis)))\n",
    "#         print(fit_params[0])\n",
    "#         ax.plot(np.array(xaxis),np.exp(fit_params[0][0]+2*np.log(np.array(xaxis))),color='tab:red',ls='-',label=r'Linear fit, $f(\\sigma)='+str(2)+r'\\sigma+'+str(fit_params[0][0])+r'$')\n",
    "#     #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "#     #print('sigma={0} with slope={1}'.format(str(k), str(fit_slope)))\n",
    "    \n",
    "#     ax.legend(loc='lower right',fontsize=15)\n",
    "#     ax.figure.savefig(load_filename+'_with_fixed_slope.png',dpi=300)\n",
    "#     ax.figure.savefig(load_filename+'_with_fixed_slope.svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]#[0.1]\n",
    "static_disorder_xx_var_arr = sorted(np.append(2e-3*np.array(range(10,200,10)),0))\n",
    "max_N = 1000#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = [max_N]#list(range(N_start,max_N+1,5))\n",
    "L_sizes = [30]\n",
    "disorder_count = 200\n",
    "is_disorder_every_step = False\n",
    "model_name = 'static_disorder_with_everystep='+str(disorder_xx_var_arr[0])+'_for_scaling_only_end_step'\n",
    "print(N_opt)\n",
    "print(static_disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the Excitations\n",
    "for cur_sys_size in L_sizes:\n",
    "    # TODO: save them with actual names\n",
    "    plot_kz_scaling_disorder_static_new_format(N_opt,cur_sys_size,N_opt[-1],model_name,rad=radi,trot=to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the saturation N_max values\n",
    "#for cur_sys_size in L_sizes:\n",
    "#    plot_sat_values(cur_sys_size,max_N)\n",
    "# Plot the saturation N_max values\n",
    "for cur_sys_size in L_sizes:\n",
    "    plot_sat_values_with_fixed_slope(cur_sys_size,max_N)\n",
    "for cur_sys_size in L_sizes:\n",
    "    plot_sat_values_with_fixed_slope(cur_sys_size,max_N,match_guess=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge results (AVOID USAGE)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Merge results of disorders for system size L and save it\n",
    "# TODO: Chnage it to a function for quick merging\n",
    "sys_size = 50\n",
    "model1 = 'nisq_parameters_low_nsteps'\n",
    "model2 = 'nisq_parameters'\n",
    "maxN1 = 28\n",
    "maxN2 = 1000\n",
    "load_filename1 = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(maxN1),model1)\n",
    "load_filename2 = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(maxN2),model2)\n",
    "yaxis_dict1 = np.load(load_filename1+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "yaxis_std_dict1 = np.load(load_filename1+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "yaxis_dict2 = np.load(load_filename2+'_endstep_data_mean_dict.npy', allow_pickle=True).item()\n",
    "yaxis_std_dict2 = np.load(load_filename2+'_endstep_data_std_dict.npy', allow_pickle=True).item()\n",
    "print(yaxis_std_dict1)\n",
    "print(yaxis_std_dict2)\n",
    "yaxis_res = dict()\n",
    "for k in yaxis_dict1.keys() & yaxis_dict2.keys():\n",
    "    temp1 = yaxis_dict1.get(k)\n",
    "    temp2 = yaxis_dict2.get(k)\n",
    "    temp1.update(temp2)\n",
    "    yaxis_res[k] = temp1\n",
    "yaxis_std_res = dict()\n",
    "for k in yaxis_std_dict1.keys() & yaxis_std_dict2.keys():\n",
    "    temp1 = yaxis_std_dict1.get(k)\n",
    "    temp2 = yaxis_std_dict2.get(k)\n",
    "    temp1.update(temp2)\n",
    "    yaxis_std_res[k] = temp1\n",
    "print(yaxis_std_res)\n",
    "result_name = 'merged_'+model2\n",
    "res_nmax = max(maxN1,maxN2)\n",
    "result_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(res_nmax),result_name)\n",
    "np.save(result_filename+'_endstep_data_mean_dict.npy',yaxis_res)\n",
    "np.save(result_filename+'_endstep_data_std_dict.npy',yaxis_std_res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reworked plots with the new $N_{steps}$ definition UPDATE FROM 27.03.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot definitions (from FloquetKibbleZurek.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Font definitions\n",
    "FONTSIZE_LEGEND = 22\n",
    "FONTSIZE_AXIS = 24\n",
    "FONTSIZE_TITLE = 24\n",
    "FONTSIZE_TICKS = 20\n",
    "\n",
    "# OLD VERSION WITH OLD FILENAMES - DO NOT USE\n",
    "#def plot_kz_scaling_disorder_deprecated(sys_size):\n",
    "#    sigmas=[]\n",
    "#    defects=[]\n",
    "#    defects_err=[]\n",
    "#\n",
    "#    #plt.tight_layout()\n",
    "#\n",
    "#    # Load data from files\n",
    "#    old_filename = 'kz_scaling_{0}system_fermion_mapping_disorder_everystep=True_optimized_new_version'.format(str(sys_size))\n",
    "#    yaxis_dict = np.load(old_filename+'_data_mean_dict.npy', allow_pickle=True).item()\n",
    "#    yaxis_std_dict = np.load(old_filename+'_data_std_dict.npy', allow_pickle=True).item()\n",
    "#    \n",
    "#    for k,yaxis in sorted(yaxis_dict.items()):\n",
    "#        sigmas.append(k)\n",
    "#        defects.append(yaxis[1:])\n",
    "#        defects_err.append(yaxis_std_dict[k][1:])\n",
    "#    \n",
    "#    #ax.figure.savefig(old_filename+'.png')\n",
    "#    #ax.figure.savefig(old_filename+'.svg')\n",
    "#    return sigmas,defects,defects_err\n",
    "\n",
    "# New data format\n",
    "def data_kz_scaling_disorder(sys_size,max_N,model_name,xaxis=None):\n",
    "    # Load data from files\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad=1_trotter=False_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "    yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "    yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "    # Choose the xaxis to be the N's of the first in the dictionary\n",
    "    if xaxis is None:\n",
    "        xaxis = list(list(yaxis_dict.values())[0].keys())\n",
    "    \n",
    "    sigmas = []\n",
    "    defects = []\n",
    "    defects_err = []\n",
    "    for k,yaxis in sorted(yaxis_dict.items()):\n",
    "        sigmas.append(k)\n",
    "        l_yaxis = []\n",
    "        l_yerr_arr = []\n",
    "        for n_opt in xaxis:\n",
    "            l_yaxis.append(yaxis[n_opt])\n",
    "            l_yerr_arr.append(yaxis_std_dict[k][n_opt])\n",
    "        defects.append(np.array(l_yaxis))\n",
    "        #defects.append(np.array(l_yaxis)*(sys_size-1)/(sys_size)+(1/(2*sys_size)))\n",
    "        #defects.append(np.array(l_yaxis)+(1/(2*sys_size)))\n",
    "        defects_err.append(np.array(l_yerr_arr))\n",
    "    \n",
    "    return np.array(xaxis),np.array(sigmas),np.array(defects),np.array(defects_err)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_zero_disorder_match(L_size_arr,max_N,model_name,xaxis=None,add1overL=False,fit_region=[]):\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_ylabel(r'$d_\\mathrm{ideal}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$N$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    is_first=True\n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    for L_size in L_size_arr:\n",
    "        if L_size<10:\n",
    "            continue\n",
    "        xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name,xaxis)\n",
    "        if is_first:\n",
    "            #ax.set_xticks(xaxis) # NO MINOR BOOL FOR NEWER MATPLOTLIB\n",
    "            is_first=False\n",
    "        for s in range(len(sigmas)):\n",
    "            cur_disorder = sigmas[s]\n",
    "            defects_corrected=np.array(defects[s])\n",
    "            ax.errorbar(xaxis,defects_corrected,ls='--',label=r'$L='+str(L_size)+r'$',marker='o',markersize=4,capsize=2,yerr=defects_err[s])\n",
    "        if max(L_size_arr)==L_size:\n",
    "            if (len(fit_region)<2) : fit_region=range(0,len(xaxis))\n",
    "            # This is the old fit that tries to fit also the expoennt b\n",
    "            #fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(xaxis[fit_region]), np.log(defects_corrected[fit_region]))\n",
    "            #a = fit_params[0][0]\n",
    "            #b = fit_params[0][1]\n",
    "            #print(\"For L_size={0} we have params={1}\".format(str(L_size),str((a,b))))\n",
    "            #ax.plot(xaxis,np.exp(a+b*np.log(xaxis)),c='r',ls='--',label = r'${0}N^{{{1}}}$'.format(str(np.exp(a))[0:5],str(b)[0:5]))\n",
    "            # Here we try to fit only the coefficient a for the -0.5 exponent  \n",
    "            fit_params = scipy.optimize.curve_fit(lambda x,a: a*x**(-0.5), xaxis[fit_region[0]:fit_region[1]], np.array(defects[s])[fit_region[0]:fit_region[1]])#, sigma=np.array(defects_err[s])[fit_region])\n",
    "            a = fit_params[0][0]\n",
    "            err = np.sqrt(fit_params[1][0][0])\n",
    "            #percent_error = max(np.exp(err)-1,1-np.exp(-err))\n",
    "            print(\"Prefactor:{0}\".format(a))\n",
    "            #print(\"Percent error:{0}\".format(percent_error))\n",
    "            print(\"{0}---{1}\".format(a-err,a+err))\n",
    "            # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "            # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "            print('standard deviation={0}'.format(str(err)))\n",
    "            ax.plot(xaxis,a*np.exp(-0.5*np.log(xaxis)),c='dimgray',lw=4,ls='--',label = r'$\\mathrm{KZ}$')#r'${0:.3f}N^{{-0.5}}$'.format(a))\n",
    "    ax.legend(loc='lower left',fontsize=FONTSIZE_LEGEND)\n",
    "    # UPDATE 27.03.22: We now plot into pdf instead of png or svg\n",
    "    name = 'figures/kz_scaling_{0}system_maxN={1}_fermion_mapping_disorder_everystep=True_new_nsteps_definition_optimized_new_version'.format(str(L_size_arr),max_N)\n",
    "    ax.figure.savefig(name+'.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return ax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the saturation values as a function of the disorder for static disorder\n",
    "def plot_sat_values_with_fixed_slope(sys_size_arr,inp_max_N,rad=1,trot=False,match_guess=False,apply_opacity_error=False):\n",
    "    \n",
    "    # Initialize the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_ylabel(r'$d_\\mathrm{disorder} = d-d_\\mathrm{ideal}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$\\sigma_\\mathrm{disorder}$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # UPDATE 27.03.22: Force match_guess\n",
    "    #if not match_guess:\n",
    "    #    raise Exception(\"we must match_guess=True as we plot here label of 0.8\\sigma^2\")\n",
    "\n",
    "    is_first = True\n",
    "    for sys_size in sys_size_arr:\n",
    "        \n",
    "            # Load data from files\n",
    "            try:\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(inp_max_N),str(rad),str(trot),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            except (FileNotFoundError):\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(inp_max_N),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            max_N_arr = list(yaxis_dict[0,0].keys())\n",
    "            for cur_max_N in max_N_arr:\n",
    "                # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "                xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "                yaxis = []\n",
    "                yerr_arr = []\n",
    "\n",
    "                # Get the every step noise\n",
    "                k_every_step_val = sorted(yaxis_dict.keys())[0][0]\n",
    "\n",
    "                for k in sorted(yaxis_dict.keys()):\n",
    "                    if k == (k_every_step_val,0):\n",
    "                        continue\n",
    "                    \n",
    "                    # Ignore negative values\n",
    "                    val_y = yaxis_dict[k][cur_max_N]-yaxis_dict[(k_every_step_val,0)][cur_max_N]\n",
    "                    if val_y <0:\n",
    "                        continue\n",
    "\n",
    "                    xaxis.append(k[1])\n",
    "                    yaxis.append(yaxis_dict[k][cur_max_N]-yaxis_dict[(k_every_step_val,0)][cur_max_N])\n",
    "                    yerr_arr.append(yaxis_std_dict[k][cur_max_N])\n",
    "                print(xaxis)\n",
    "                print(yaxis)\n",
    "                print(yerr_arr)\n",
    "                if not apply_opacity_error:\n",
    "                    ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=6,capsize=5,lw=3,yerr=yerr_arr,label=r'$L='+str(sys_size)+r', N = '+str(cur_max_N)+r'$')\n",
    "                else:\n",
    "                    markers, caps, bars = ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=6,capsize=5,lw=3,yerr=yerr_arr,label=r'$L='+str(sys_size)+r', N = '+str(cur_max_N)+r'$')\n",
    "                    [bar.set_alpha(0.35) for bar in bars]\n",
    "                    [cap.set_alpha(0.35) for cap in caps]\n",
    "                # Fit for the maximum length\n",
    "                if ((sys_size==30 and cur_max_N==20) or (len(sys_size_arr)>1 and sys_size == sys_size_arr[2])) and is_first:\n",
    "                    is_first=False\n",
    "                    if match_guess:\n",
    "                        guess_coe = 1\n",
    "                        guess_co = np.log(guess_coe)\n",
    "                        ax.plot(np.array(xaxis),np.exp(guess_co+2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma_\\mathrm{disorder})=\\sigma_\\mathrm{disorder}^2$')\n",
    "                    else:\n",
    "                        print(\"START FOR FOR N={0}\".format(str(cur_max_N)))\n",
    "                        fit_params = scipy.optimize.curve_fit(lambda x,a: a*x**2, np.array(xaxis), np.array(yaxis),sigma=np.array(yerr_arr))\n",
    "                        print(fit_params)\n",
    "                        a = fit_params[0][0]\n",
    "                        err = np.sqrt(np.diag(fit_params[1]))[0]\n",
    "                        #percent_error = max(np.exp(err)-1,1-np.exp(-err))\n",
    "                        #print(percent_error)\n",
    "                        print(\"Prefactor={0}\".format(str(a)))\n",
    "                        print(\"{0}---{1}\".format(a-err,a+err))\n",
    "                        # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "                        # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "                        print('standard deviation={0}'.format(str(err)))\n",
    "                        # The standard deviation of the parameters are the square roots of the diagonal elements of pcov=fit_params[1]. See https://stackoverflow.com/a/21844726\n",
    "                        ax.plot(np.array(xaxis),a*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma_\\mathrm{{disorder}})={0:.2f}\\sigma_\\mathrm{{disorder}}^2$'.format(a))\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_new_nsteps_definition_corrected_noise_rad={2}_trotter={3}_{4}'.format(str(sys_size_arr),str(max_N_arr),str(rad),str(trot),model_name[:5])\n",
    "    if match_guess:\n",
    "        load_filename += '_coe=1'\n",
    "    \n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the saturation values as a function of the disorder for static disorder\n",
    "def plot_sat_values_with_fixed_slope_noise(sys_size_arr,inp_max_N,rad=1,trot=False,match_guess=False):\n",
    "    \n",
    "    # Initialize the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_ylabel(r'$d_\\mathrm{noise} = d-d_\\mathrm{ideal}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$\\sigma_\\mathrm{noise}$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # UPDATE 27.03.22: Force match_guess\n",
    "    #if not match_guess:\n",
    "    #    raise Exception(\"we must match_guess=True as we plot here label of 0.8\\sigma^2\")\n",
    "\n",
    "    is_first = True\n",
    "    for sys_size in sys_size_arr:\n",
    "        \n",
    "            # Load data from files\n",
    "            try:\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(inp_max_N),str(rad),str(trot),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            except (FileNotFoundError):\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(inp_max_N),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            max_N_arr = list(yaxis_dict[0,0].keys())\n",
    "            for cur_max_N in max_N_arr:\n",
    "                # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "                xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "                yaxis = []\n",
    "                yerr_arr = []\n",
    "\n",
    "                # Get the static disorder\n",
    "                k_dis_val = sorted(yaxis_dict.keys())[0][1]\n",
    "                print(k_dis_val)\n",
    "\n",
    "                for k in sorted(yaxis_dict.keys()):\n",
    "                    if k == (0,k_dis_val):\n",
    "                        continue\n",
    "                    \n",
    "                    # Ignore negative values\n",
    "                    val_y = yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N]\n",
    "                    if val_y <0:\n",
    "                        continue\n",
    "\n",
    "                    xaxis.append(k[0])\n",
    "                    yaxis.append(yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N])\n",
    "                    yerr_arr.append(yaxis_std_dict[k][cur_max_N])\n",
    "                print(xaxis)\n",
    "                print(yaxis)\n",
    "                print(yerr_arr)\n",
    "                ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=6,capsize=5,lw=3,yerr=yerr_arr,label=r'$L='+str(sys_size)+r', N = '+str(cur_max_N)+r'$')\n",
    "                \n",
    "                if is_first and sys_size==max(sys_size_arr):\n",
    "                    is_first=False\n",
    "                    if match_guess:\n",
    "                        guess_coe = 0.8\n",
    "                        guess_co = np.log(guess_coe)\n",
    "                        ax.plot(np.array(xaxis),np.exp(guess_co+2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma)=0.8\\sigma^2$')\n",
    "                    else:\n",
    "                        fit_params = scipy.optimize.curve_fit(lambda x,a: a*cur_max_N*x**2, np.array(xaxis), np.array(yaxis),sigma=np.array(yerr_arr))\n",
    "                        a = fit_params[0][0]\n",
    "                        err = np.sqrt(fit_params[1][0][0])\n",
    "                        #percent_error = max(np.exp(err)-1,1-np.exp(-err))\n",
    "                        #print(percent_error)\n",
    "                        print(\"Prefactor={0}\".format(str(a)))\n",
    "                        print(\"{0}---{1}\".format(a-err,a+err))\n",
    "                        # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "                        # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "                        print('standard deviation={0}'.format(str(err)))\n",
    "                        print(fit_params[0])\n",
    "\n",
    "                        ax.plot(np.array(xaxis),a*cur_max_N*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma_\\mathrm{{noise}})={0:.2f}N\\sigma_\\mathrm{{noise}}^2$'.format(a))\n",
    "\n",
    "                        # UPDATE 06.08.22: Without fit\n",
    "                        #ax.plot(np.array(xaxis),2*cur_max_N*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma_\\mathrm{noise})=2 N\\sigma_\\mathrm{noise}^2$')\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_new_nsteps_definition_corrected_noise_rad={2}_trotter={3}_{4}'.format(str(sys_size_arr),str(max_N_arr),str(rad),str(trot),model_name[:5])     \n",
    "    if match_guess:\n",
    "        load_filename += '_coe='+str(guess_coe)\n",
    "    \n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the saturation values as a function of the noise (every step)\n",
    "def plot_sat_values_with_fixed_slope_noise_nmax(sys_size,cur_nmax,rad=1,trot=False,match_guess=False,cap_nmax=0):\n",
    "    # Initialize the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_ylabel(r'$d_\\mathrm{noise} = d-d_\\mathrm{ideal}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$N$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # UPDATE 27.03.22: Force match_guess\n",
    "    #if not match_guess:\n",
    "    #    raise Exception(\"we must match_guess=True as we plot here label of 0.8\\sigma^2\")\n",
    "\n",
    "    is_first = True\n",
    "    try:\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(cur_nmax),str(rad),str(trot),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "    except (FileNotFoundError):\n",
    "        load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(cur_nmax),model_name)\n",
    "        yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "        yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "    max_N_arr = list(yaxis_dict[0,0].keys())\n",
    "    # Get the static disorder\n",
    "    k_dis_val = sorted(yaxis_dict.keys())[0][1]\n",
    "    print(max_N_arr)\n",
    "\n",
    "    val_fit = None\n",
    "    for k in sorted(yaxis_dict.keys(),reverse=True):\n",
    "        if k == (0,k_dis_val):\n",
    "            continue\n",
    "        # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "        xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "        yaxis = []\n",
    "        yerr_arr = []\n",
    "        for cur_max_N in max_N_arr:\n",
    "            if cur_max_N < cap_nmax:\n",
    "                continue\n",
    "            # Ignore negative values\n",
    "            val_y = yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N]\n",
    "            if val_y <0:\n",
    "                continue\n",
    "            \n",
    "            xaxis.append(cur_max_N)\n",
    "            yaxis.append(yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N])\n",
    "            yerr_arr.append(yaxis_std_dict[k][cur_max_N])\n",
    "        #print(xaxis)\n",
    "        #print(yaxis)\n",
    "        #print(yerr_arr)\n",
    "        ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=6,capsize=5,lw=3,yerr=yerr_arr,label=r'$L='+str(sys_size)+r'$, $\\sigma_\\mathrm{noise} = '+str(k[0])+r'$')\n",
    "        \n",
    "        cur_sigma = k[0]\n",
    "        if cur_sigma>=1e-3 and is_first:\n",
    "            is_first=False\n",
    "            #if match_guess:\n",
    "            #    guess_coe = 0.8\n",
    "            #   guess_co = np.log(guess_coe)\n",
    "            #    ax.plot(np.array(xaxis),np.exp(guess_co+2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma)=0.8\\sigma^2$')\n",
    "            #else:\n",
    "            # the sigma of the noise\n",
    "            \n",
    "            fit_params = scipy.optimize.curve_fit(lambda x,a: a*x*cur_sigma**2, np.array(xaxis), np.array(yaxis),sigma=np.array(yerr_arr))\n",
    "            print(\"Sigma to fit:{0}\".format(cur_sigma))\n",
    "            a = fit_params[0][0]\n",
    "            err = np.sqrt(fit_params[1][0][0])\n",
    "            #percent_error = max(np.exp(err)-1,1-np.exp(-err))\n",
    "            #print(percent_error)\n",
    "            print(\"Prefactor={0}\".format(str(a)))\n",
    "            print(\"{0}---{1}\".format(a-err,a+err))\n",
    "            # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "            # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "            print('standard deviation={0}'.format(str(err)))\n",
    "\n",
    "            val_fit = a\n",
    "            ax.plot(np.array(xaxis),a*np.array(xaxis)*cur_sigma**2,color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(N)={0:.2f}N\\sigma_\\mathrm{{noise}}^2$'.format(a))\n",
    "        # Adding aqua marine marker lines to all other lines\n",
    "        else:\n",
    "            cur_sigma = k[0]\n",
    "            ax.plot(np.array(xaxis),val_fit*cur_sigma**2*np.array(xaxis),color='aquamarine',zorder=-10,ls='-',lw=10)\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_new_nsteps_definition_corrected_noise_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(cur_nmax),str(rad),str(trot),model_name[:5])\n",
    "\n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the saturation values as a function of the disorder for static disorder\n",
    "def plot_sat_values_with_fixed_slope_noise_alternative(sys_size_arr,inp_max_N,rad=1,trot=False,match_guess=False):\n",
    "    \n",
    "    # Initialize the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_ylabel(r'$d_\\mathrm{noise} = d-d_\\mathrm{ideal}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$\\sigma_\\mathrm{noise}$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # UPDATE 27.03.22: Force match_guess\n",
    "    #if not match_guess:\n",
    "    #    raise Exception(\"we must match_guess=True as we plot here label of 0.8\\sigma^2\")\n",
    "\n",
    "    is_first = True\n",
    "    for sys_size in sys_size_arr:\n",
    "        \n",
    "            # Load data from files\n",
    "            try:\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad={2}_trotter={3}_{4}'.format(str(sys_size),str(inp_max_N),str(rad),str(trot),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            except (FileNotFoundError):\n",
    "                load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_{2}'.format(str(sys_size),str(inp_max_N),model_name)\n",
    "                yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "                yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "            max_N_arr = list(yaxis_dict[0,0].keys())\n",
    "            for cur_max_N in max_N_arr:\n",
    "                # TODO: Read the correct files and remove the 0 STATIC disorder\n",
    "                xaxis = []#np.array([k[1] for k in yaxis_dict.keys()])\n",
    "                yaxis = []\n",
    "                yerr_arr = []\n",
    "\n",
    "                # Get the static disorder\n",
    "                k_dis_val = sorted(yaxis_dict.keys())[0][1]\n",
    "                print(k_dis_val)\n",
    "\n",
    "                for k in sorted(yaxis_dict.keys()):\n",
    "                    if k == (0,k_dis_val):\n",
    "                        continue\n",
    "                    \n",
    "                    # Ignore negative values\n",
    "                    val_y = yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N]\n",
    "                    if val_y <0:\n",
    "                        continue\n",
    "\n",
    "                    xaxis.append(k[0])\n",
    "                    yaxis.append(yaxis_dict[k][cur_max_N]-yaxis_dict[(0,k_dis_val)][cur_max_N])\n",
    "                    yerr_arr.append(yaxis_std_dict[k][cur_max_N])\n",
    "                print(xaxis)\n",
    "                print(yaxis)\n",
    "                print(yerr_arr)\n",
    "                ax.errorbar(np.array(xaxis),np.array(yaxis),ls='--',marker='o',markersize=6,capsize=5,lw=3,yerr=yerr_arr,label=r'$L='+str(sys_size)+r', N = '+str(cur_max_N)+r'$')\n",
    "                \n",
    "                if is_first:\n",
    "                    is_first=False\n",
    "                    #if match_guess:\n",
    "                    #    guess_coe = 0.8\n",
    "                    #    guess_co = np.log(guess_coe)\n",
    "                    #    ax.plot(np.array(xaxis),np.exp(guess_co+2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma)=0.8\\sigma^2$')\n",
    "                    #else:\n",
    "\n",
    "                    # UPDATE 08.06.2022 - DON'T FIT\n",
    "                    #fit_params = scipy.optimize.curve_fit(lambda x,a: a+2*x, np.log(np.array(xaxis)), np.log(np.array(yaxis)))\n",
    "                    #print(fit_params[0])\n",
    "                    #val_fit = np.exp(fit_params[0][0])/cur_max_N\n",
    "                    #ax.plot(np.array(xaxis),np.exp(fit_params[0][0]+2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma)='+str(val_fit)+r'\\sigma^2 N_{max}$')\n",
    "                    # WITHOUT FIT:\n",
    "                    ax.plot(np.array(xaxis),2*cur_max_N*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10,label=r'$f(\\sigma_\\mathrm{noise})=2N\\sigma_\\mathrm{noise}^2 $')\n",
    "                else:\n",
    "                    # WITH FIT:\n",
    "                    #ax.plot(np.array(xaxis),val_fit*cur_max_N*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10)\n",
    "                    # WITHOUT:\n",
    "                    ax.plot(np.array(xaxis),2*cur_max_N*np.exp(2*np.log(np.array(xaxis))),color='aquamarine',zorder=-10,ls='-',lw=10)\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_new_nsteps_definition_corrected_noise_rad={2}_trotter={3}_{4}'.format(str(sys_size_arr),str(max_N_arr),str(rad),str(trot),model_name[:5])     \n",
    "    #if match_guess:\n",
    "    #    load_filename += '_coe='+str(guess_coe)\n",
    "    \n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT NEEDED! FIG.X: No disorder, $L=100$, $N_{max}=100$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 150\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,2))\n",
    "L_sizes = [150]\n",
    "disorder_count = 1 # There is no disorder so we take only one sample\n",
    "model_name = 'no_disorder_old'\n",
    "print(N_opt)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "# Load the data\n",
    "max_N =149\n",
    "ax=plot_zero_disorder_match([L_sizes[0]],max_N,model_name,add1overL=False,fit_region=[10,74])\n",
    "#ax.figure.savefig('figures/final_overleaf_definitions_10_06_2022/FIG2.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECIDED TO NOT SHOW: FIG.3(c): Part 1, Varying static disorder, $L = 20,45,70,95$, $N_{max}= 10$. Fit of $0.8\\sigma^2$ plotted above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]#[0.1]\n",
    "static_disorder_xx_var_arr = sorted(np.append(np.logspace(-3,-0.6,15),0))#sorted(np.append(2e-3*np.array(range(10,200,10)),0))\n",
    "max_N = 10#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = [max_N]#list(range(N_start,max_N+1,5))\n",
    "L_sizes = [20,45,70,95]\n",
    "disorder_count = 500\n",
    "is_disorder_every_step = False\n",
    "model_name = 'static_disorder_with_everystep='+str(disorder_xx_var_arr[0])+'_for_scaling_only_end_step_disorder_count=500'\n",
    "print(N_opt)\n",
    "print(static_disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_AXIS = 30\n",
    "FONTSIZE_TICKS = 26\n",
    "\n",
    "L_sizes_arr = [20,45,70,95]\n",
    "max_N_arr = 10\n",
    "#model_name = 'static_disorder_with_everystep=0_for_scaling_only_end_step_disorder_count=500'\n",
    "# TODO: Enable this after testing\n",
    "plot_sat_values_with_fixed_slope(L_sizes_arr,max_N_arr,match_guess=False)\n",
    "# TEST NOW WHAT IS THE FITTED COEFFICIENT AND TRY TO ESTIMATE THE DEVIATION\n",
    "#plot_sat_values_with_fixed_slope(L_sizes_arr,max_N_arr,match_guess=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIG.3(d): Part 2, Varying static disorder, $L=30$, $N_{max} = 20,100,200,400$. Fit of $0.8\\sigma^2$ plotted above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]#[0.1]\n",
    "static_disorder_xx_var_arr = sorted(np.append(np.logspace(-3,-0.6,20),0))#sorted(np.append(2e-3*np.array(range(10,200,10)),0))\n",
    "max_N = 1000#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = [20,100,200,400]\n",
    "L_sizes = [30]\n",
    "# Was 100 but updated the disorder count to 500 after the suggestion of Bram 21.07.22\n",
    "disorder_count = 500#100\n",
    "is_disorder_every_step = False\n",
    "model_name = 'static_disorder_with_everystep='+str(disorder_xx_var_arr[0])+'_for_scaling_only_end_step_disorder_count='+str(disorder_count)\n",
    "print(N_opt)\n",
    "print(static_disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_AXIS = 30\n",
    "FONTSIZE_TICKS = 26\n",
    "\n",
    "L_sizes_arr = [30]\n",
    "max_N_arr = [20,100,200,400]\n",
    "#model_name = 's_d_w_s='+str(disorder_xx_var_arr[0])+'_c=100'\n",
    "plot_sat_values_with_fixed_slope(L_sizes_arr,max(max_N_arr),match_guess=False,apply_opacity_error=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.2: Varying each step disorder $\\sigma$, varying $N_{steps}$, $L=70$ with theoretical fits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "# Update 31.05.2022: Remove the 0.3 and 0.5 as they don't really combine well with the ansatz scaling\n",
    "disorder_xx_var_arr = [0,1e-2,3e-2,5e-2,1e-1]#,3e-1,5e-1]#,1]\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 100#30\n",
    "N_start = 1\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "model_name = 'everystep_disorder'\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "L_sizes = [120]\n",
    "disorder_count = 10\n",
    "print(N_opt)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define xaxis and plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "   \n",
    "counts_points = 100\n",
    "N_start = 1\n",
    "max_N = 100\n",
    "L_size = 120\n",
    "xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name)\n",
    "sigmas = [x[0] for x in sigmas]\n",
    "print(xaxis)\n",
    "#plt.plot(xaxis,0.35*xaxis**-0.5,'g--')\n",
    "for s in range(len(sigmas)):\n",
    "    if sigmas[s] == 0:\n",
    "        fit_region=[50,99]\n",
    "        if (len(fit_region)<2) : fit_region=range(0,len(xaxis))\n",
    "        if (len(fit_region)==2) : fit_region=range(fit_region[0],fit_region[1]+1)\n",
    "        #fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(xaxis[fit_region]), np.log(np.array(defects[s])[fit_region]))\n",
    "        #a = fit_params[0][0]\n",
    "        #b = fit_params[0][1]\n",
    "        #print((a,b))\n",
    "        #ax.plot(xaxis,np.exp(a+b*np.log(xaxis)),c='r',ls='--',lw=5,label = r'$N^{{{0}}}$'.format(str(b)[0:5]))\n",
    "\n",
    "        #fit_params = scipy.optimize.curve_fit(lambda x,a,b: a-0.5*x, np.log(xaxis[fit_region]), np.log(np.array(defects[s])[fit_region]))\n",
    "        #a = fit_params[0][0]\n",
    "        ##b = fit_params[0][1]\n",
    "        #print(a)\n",
    "        #ax.plot(xaxis,np.exp(a-0.5*np.log(xaxis)),c='r',ls='--',lw=5,label = r'${0}N^{{-0.5}}$'.format(str(np.exp(a))[0:5]))\n",
    "\n",
    "\n",
    "        fit_params = scipy.optimize.curve_fit(lambda x,a: a*x**(-0.5), xaxis[fit_region], np.array(defects[s])[fit_region])\n",
    "        a = fit_params[0][0]\n",
    "        err = np.sqrt(fit_params[1][0][0])\n",
    "        #percent_error = max(np.exp(err)-1,1-np.exp(-err))\n",
    "        print(\"Prefactor:{0}\".format(a))\n",
    "        #print(\"Percent error:{0}\".format(percent_error))\n",
    "        print(\"{0}---{1}\".format(a-err,a+err))\n",
    "        # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "        # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "        print('standard deviation={0}'.format(str(err)))\n",
    "        #b = fit_params[0][1]\n",
    "        #print(a)\n",
    "        ax.plot(xaxis,a*np.exp(-0.5*np.log(xaxis)),c='dimgray',ls='--',lw=5,label = r'$\\mathrm{KZ}$')#r'${0:.3f}N^{{-0.5}}$'.format(a))\n",
    "\n",
    "        # Plot also the 0 line\n",
    "        #plt.errorbar(times,np.array(defects[s]),ls='--',label=r'$\\sigma='+str(sigmas[s])+r'$',marker='o',markersize=4,capsize=2,yerr=defects_err[s])\n",
    "        plt.errorbar(xaxis,np.array(defects[s]),ls='--',label=r'$\\sigma_\\mathrm{noise}=0$',marker='o',markersize=4,capsize=2,yerr=defects_err[s])\n",
    "\n",
    "        #plt.plot(times,np.array(defects[0])+2*times*sigmas[s]**2,'k--',zorder=10) \n",
    "    else:\n",
    "        fit_region=range(10,100)\n",
    "        plt.errorbar(xaxis,np.array(defects[s]),ls='--',label=r'$\\sigma_\\mathrm{noise}='+str(sigmas[s])+r'$',marker='o',markersize=4,capsize=2,yerr=defects_err[s])\n",
    "        # TODO: Check if we fit hee corresponding to the fit region????\n",
    "        # Fit only for the lowest sigma\n",
    "        if sigmas[s] == 1e-2:\n",
    "            fit_params = scipy.optimize.curve_fit(lambda x,a: a*x*sigmas[s]**2, xaxis[fit_region], np.array(defects[s]-defects[0])[fit_region],sigma=np.array(defects_err[s])[fit_region])\n",
    "            a = fit_params[0][0]\n",
    "            err = np.sqrt(fit_params[1][0][0])\n",
    "            #percent_error = (a+err)/a-1\n",
    "            print('~~~~~~~~~ START sigma={0}~~~~~~~~~~~~~'.format(str(sigmas[s])))\n",
    "            print(\"Prefactor:{0}\".format(a))\n",
    "            #print(\"Percent error:{0}\".format(percent_error))\n",
    "            print(\"{0}---{1}\".format(fit_params[0][0]-err,fit_params[0][0]+err))\n",
    "            # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "            # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "            print('standard deviation={0}'.format(str(np.sqrt(np.diag(fit_params[1])))))\n",
    "\n",
    "\n",
    "        plt.plot(xaxis,np.array(defects[0])+a*xaxis*sigmas[s]**2,'k--',zorder=3) \n",
    "#Set log scale\n",
    "plt.yscale('log')\n",
    "plt.xscale('log') \n",
    "\n",
    "ax.set_yticks([0.02,0.1,0.5])\n",
    "ax.set_yticklabels([str(0.02),str(0.1),str(0.5)])\n",
    "plt.legend(loc=\"lower left\",fontsize=FONTSIZE_LEGEND-1,ncol=2)\n",
    "#plt.set_xticks(xaxis) # NO MINOR FLAG FOR NEWER MATPLOTLIB\n",
    "ax.set_ylabel(r'$d$',fontsize=FONTSIZE_AXIS)\n",
    "ax.set_xlabel(r'$N$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "\n",
    "\n",
    "# Get this figure without the x and y limits\n",
    "#plt.xlim([4,100])\n",
    "plt.ylim([0.02,0.6])\n",
    "plt.xticks(fontsize=FONTSIZE_TICKS)\n",
    "plt.yticks(fontsize=FONTSIZE_TICKS)\n",
    "#plt.savefig('figures/final_overleaf_definitions_10_06_2022/FIG2.pdf')\n",
    "plt.savefig('figures/final_overleaf_definitions_13_09_2022/FIG2.pdf')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.4: Optimal number of steps with theoretical fit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "# Update 31.05.2022: Remove the 0.3 and 0.5 as they don't really combine well with the ansatz scaling\n",
    "disorder_xx_var_arr = sorted(np.append(np.logspace(-3,-0.3,20),0))#,3e-1,5e-1]#,1]\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 2000#30\n",
    "N_start = 1\n",
    "N_opt = list(range(N_start,10,1))+list(range(10,100,2))+list(range(100,1000,40))+list(range(1000,max_N+1,200))\n",
    "model_name = 'everystep_disorder_for_optimal'\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "L_sizes = [20,30]#[50]\n",
    "disorder_count = 10\n",
    "print(N_opt)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "L_sizes_arr = [] # UNUSED VAR?\n",
    "max_N_arr = [] # UNUSED VAR?\n",
    "# Import data\n",
    "model_name = 'everystep_disorder_for_optimal'\n",
    "N_start = 1\n",
    "max_N = 2000#740#591\n",
    "#times = np.linspace(N_start,max_N,counts_points) # 1...100\n",
    "L_size_arr = [20,30,50]#80#150\n",
    "\n",
    "\n",
    "# Get minimas and plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "color_arr = [\"tab:red\",\"tab:blue\",\"tab:green\"]\n",
    "for i in range(len(L_size_arr)):\n",
    "    L_size = L_size_arr[i]\n",
    "    xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name)\n",
    "\n",
    "    sigmas = [x[0] for x in sigmas]\n",
    "    minima=[]\n",
    "    lower_error = []\n",
    "    upper_error = []\n",
    "    print(sigmas)\n",
    "    for s in range(len(sigmas)):\n",
    "        # Skip 0,1e-3 and the maximal sigma\n",
    "        if s==0 or s==len(sigmas)-1 or s==len(sigmas)-2:\n",
    "            print(sigmas[s])\n",
    "            sigmas[s]=-1\n",
    "            continue\n",
    "        minimum=np.min(defects[s])\n",
    "        # Here we generate error bars by adding 5% error to the minimum value\n",
    "        min_err_limit = 1.05*minimum\n",
    "        # We check the min/max xaxis and draw an error bar accordingly\n",
    "        res_limits = xaxis[np.where(defects[s]<=min_err_limit)]\n",
    "        res = xaxis[np.where(defects[s]==minimum)][0]\n",
    "        #print((min(res_limits),max(res_limits)))\n",
    "        \n",
    "        low_err = abs(min(res_limits)-res)\n",
    "        high_err=abs(max(res_limits)-res)\n",
    "        if low_err==0 and high_err==0:\n",
    "            print(sigmas[s])\n",
    "            sigmas[s]=-1\n",
    "            continue\n",
    "            \n",
    "        minima.append(res)\n",
    "        lower_error.append(low_err)\n",
    "        upper_error.append(high_err)\n",
    "    sigmas = [s for s in sigmas if s!=-1]\n",
    "    print(minima)\n",
    "    # From https://stackoverflow.com/a/62525717/5774432\n",
    "    asymmetric_error = np.array(list(zip(lower_error, upper_error))).T\n",
    "\n",
    "    plt.errorbar(sigmas,minima,yerr=asymmetric_error,c=color_arr[i],fmt='o',ms=8,capsize=8,markeredgewidth=3, elinewidth=3,label=r'$L={0}$'.format(str(L_size)))\n",
    "    # Generate Fit\n",
    "    err_fix = np.array([max(asymmetric_error[0][i],asymmetric_error[1][i]) for i in range(len(asymmetric_error[0]))])\n",
    "    #print(err_fix)\n",
    "    fit_params = scipy.optimize.curve_fit(lambda x,a: a*x**(-4.0/3.0), np.array(sigmas)[3:-6], np.array(minima)[3:-6],sigma=err_fix[3:-6])\n",
    "    a = fit_params[0][0]\n",
    "\n",
    "    print(\"~~~~~~~~~~~~~ for L={0}~~~~~~~~~~~~~~\".format(str(L_size)))\n",
    "    err = np.sqrt(fit_params[1][0][0])\n",
    "    print(\"Prefactor:{0}\".format(a))\n",
    "    print(\"{0}---{1}\".format(a-err,a+err))\n",
    "    # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "    # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "    print('standard deviation={0}'.format(err))\n",
    "    \n",
    "    #if L_size==20:\n",
    "    #    plt.plot(sigmas,a*np.exp((-4.0/3.0)*np.log(sigmas)),c=color_arr[i],lw=5,label=r'$ {0:.2f}\\sigma_\\mathrm{{noise}}^{{-\\frac{{4}}{{3}}}}$'.format(a))\n",
    "    #else:\n",
    "    #    plt.plot(sigmas,a*np.exp((-4.0/3.0)*np.log(sigmas)),c=color_arr[i],lw=5,label=r'$ {0:.3f}\\sigma_\\mathrm{{noise}}^{{-\\frac{{4}}{{3}}}}$'.format(a))\n",
    "    if L_size == 50:\n",
    "        plt.plot(sigmas,a*np.exp((-4.0/3.0)*np.log(sigmas)),c=color_arr[i],zorder=-10,lw=4,ls='--',label=r'$ {0:.3f}\\sigma_\\mathrm{{noise}}^{{-\\frac{{4}}{{3}}}}$'.format(a))\n",
    "#plt.plot(sigmas,0.8/np.array(sigmas)**(4/3),label='$ 0.8\\sigma_\\mathrm{noise}^{-4/3}$')\n",
    "#plt.ylim([1.5,200])\n",
    "#plt.xlim([0.001,0.01])\n",
    "plt.xlabel(r'$\\sigma_\\mathrm{noise}$',fontsize=FONTSIZE_AXIS)\n",
    "plt.ylabel(r'$N_{\\rm opt}$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(fontsize=FONTSIZE_TICKS)\n",
    "plt.yticks(fontsize=FONTSIZE_TICKS)\n",
    "\n",
    "#plt.xlim((10**(-3),max(sigmas)+0.05))\n",
    "\n",
    "plt.legend(loc=\"lower left\",fontsize=FONTSIZE_LEGEND,ncol=1)\n",
    "\n",
    "#plt.savefig('figures/final_overleaf_definitions_10_06_2022/FIG4.pdf')\n",
    "plt.savefig('figures/final_overleaf_definitions_13_09_2022/FIG4.pdf')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW REFEREE Q: Fig. X: Optimal number of steps as a function of $L$ for $\\sigma_{\\rm noise}=1e^{-1}$ "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "# As Emanuele suggested we try \\sigma_noise=\\sigma_disorder=0.1 which is in the experimental regime\n",
    "disorder_xx_var_arr = [0.1]\n",
    "static_disorder_xx_var_arr = [0.1]\n",
    "max_N = 15 # 120 is derived from Fig. 4 above that we see N_opt to be ~100 for \\sigma_{noise}=0.1. We don't think that disorder changes it much.\n",
    "N_start = 1\n",
    "N_opt = list(range(N_start,max_N+1,1))#list(range(N_start,10,1))+list(range(10,100,2))\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "L_sizes = [4,5,6]#list(range(5,100,5))\n",
    "disorder_count = 1000\n",
    "model_name = 'optimal_for_varying_sys_sizes_count={0}'.format(str(disorder_count))\n",
    "print(N_opt)\n",
    "print(L_sizes)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import data\n",
    "model_name = 'optimal_for_varying_sys_sizes_count=1000'\n",
    "max_N = 15#740#591\n",
    "#times = np.linspace(N_start,max_N,counts_points) # 1...100\n",
    "L_size_arr = [4,5,6]#range(10,100,5)\n",
    "\n",
    "# Get minimas and plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "color_arr = [\"tab:red\",\"tab:blue\",\"tab:green\"]\n",
    "minima=[]\n",
    "lower_error = []\n",
    "upper_error = []\n",
    "for i in range(len(L_size_arr)):\n",
    "    L_size = L_size_arr[i]\n",
    "    xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name)\n",
    "\n",
    "    sigmas = [x[0] for x in sigmas]\n",
    "\n",
    "    print(sigmas)\n",
    "\n",
    "    for s in range(len(sigmas)):\n",
    "        # Skip 0,1e-3 and the maximal sigma\n",
    "        #if s==0 or s==len(sigmas)-1 or s==len(sigmas)-2:\n",
    "        #    print(sigmas[s])\n",
    "        #    sigmas[s]=-1\n",
    "        #    continue\n",
    "        minimum=np.min(defects[s])\n",
    "        # Here we generate error bars by adding 5% error to the minimum value\n",
    "        min_err_limit = 1.05*minimum\n",
    "        # We check the min/max xaxis and draw an error bar accordingly\n",
    "        res_limits = xaxis[np.where(defects[s]<=min_err_limit)]\n",
    "        res = xaxis[np.where(defects[s]==minimum)][0]\n",
    "        #print((min(res_limits),max(res_limits)))\n",
    "        \n",
    "        low_err = abs(min(res_limits)-res)\n",
    "        high_err=abs(max(res_limits)-res)\n",
    "        #if low_err==0 and high_err==0:\n",
    "        #    print(\"ERROR! NO ERROR BARS!\")\n",
    "        #    print(sigmas[s])\n",
    "        #    sigmas[s]=-1\n",
    "        #    continue\n",
    "            \n",
    "        minima.append(res)\n",
    "        lower_error.append(low_err)\n",
    "        upper_error.append(high_err)\n",
    "    sigmas = [s for s in sigmas if s!=-1]\n",
    "    print(minima)\n",
    "# From https://stackoverflow.com/a/62525717/5774432\n",
    "asymmetric_error = np.array(list(zip(lower_error, upper_error))).T\n",
    "\n",
    "# Generate Fit\n",
    "syn_error = np.array([max(asymmetric_error[0][i],asymmetric_error[1][i]) for i in range(len(asymmetric_error[0]))])\n",
    "\n",
    "plt.errorbar(L_size_arr,minima,yerr=asymmetric_error,fmt='o',ms=8,capsize=8,markeredgewidth=3, elinewidth=3,label=r'$\\sigma_{\\rm noise}=\\sigma_{\\rm disorder}=0.1$')\n",
    "\n",
    "\n",
    "\n",
    "#plt.ylim([1.5,200])\n",
    "#plt.xlim([0.001,0.01])\n",
    "plt.xlabel(r'$L$',fontsize=FONTSIZE_AXIS)\n",
    "plt.ylabel(r'$N_{\\rm opt}$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.xticks(fontsize=FONTSIZE_TICKS)\n",
    "plt.yticks(fontsize=FONTSIZE_TICKS)\n",
    "\n",
    "plt.legend(loc=\"lower right\",fontsize=FONTSIZE_LEGEND,ncol=1)\n",
    "\n",
    "#plt.savefig('figures/final_overleaf_definitions_10_06_2022/FIG4.pdf')\n",
    "#plt.savefig('figures/final_overleaf_definitions_13_09_2022/REFQ.pdf')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW REFEREE Q: Fig. X2: Number of defects d vs N_{steps} to match with experiment for disorder=noise=0.1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0.1]\n",
    "static_disorder_xx_var_arr = [0.1]\n",
    "max_N = 30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "L_sizes = [4,5,6]\n",
    "disorder_count = 5000 # There is no disorder so we take only one sample\n",
    "model_name = 'noise=0.1_disorder=0.1_experimental_sys_sizes_count={0}'.format(disorder_count)\n",
    "print(N_opt)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numsets=-1\n",
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius,setNum=numsets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "# Define xaxis and plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "N_start = 1\n",
    "max_N = 30\n",
    "L_sizes = [4,5,6]\n",
    "\n",
    "\n",
    "#print(xaxis)\n",
    "#plt.plot(xaxis,0.35*xaxis**-0.5,'g--')\n",
    "for i in range(len(L_sizes)):\n",
    "    L_size = L_sizes[i]\n",
    "    # Load the data\n",
    "    xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name)\n",
    "    print(xaxis)\n",
    "    #print(defects)\n",
    "    #fit_region=range(10,100)\n",
    "    for s in range(len(sigmas)):\n",
    "        print(defects_err[s]/np.sqrt(5000))\n",
    "        print(sigmas[s])\n",
    "        if sigmas[s][0] == 0 or sigmas[s][1]==0:\n",
    "            continue\n",
    "        plt.errorbar(xaxis,np.array(defects[s]),ls='--',label=r'$L={0}$'.format(str(L_size)),marker='o',markersize=4,capsize=6,yerr=defects_err[s]/np.sqrt(5000))\n",
    "\n",
    "    # This is the fit\n",
    "    #plt.plot(xaxis,np.array(defects[0])+a*xaxis*sigmas[s]**2,'k--',zorder=3) \n",
    "#Set log scale\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log') \n",
    "\n",
    "#ax.set_yticks([0.02,0.1,0.5])\n",
    "#ax.set_yticklabels([str(0.02),str(0.1),str(0.5)])\n",
    "plt.legend(loc=\"lower right\",fontsize=FONTSIZE_LEGEND,ncol=1)\n",
    "#plt.set_xticks(xaxis) # NO MINOR FLAG FOR NEWER MATPLOTLIB\n",
    "ax.set_ylabel(r'$d$',fontsize=FONTSIZE_AXIS)\n",
    "ax.set_xlabel(r'$N$',fontsize=FONTSIZE_AXIS)\n",
    "ax.text(2,0.44,r'$\\sigma_\\mathrm{disorder}=\\sigma_\\mathrm{noise}=0.1$',fontsize=FONTSIZE_LEGEND+5)\n",
    "\n",
    "\n",
    "# Get this figure without the x and y limits\n",
    "#plt.xlim([4,100])\n",
    "#plt.ylim([0.02,0.6])\n",
    "plt.xticks(fontsize=FONTSIZE_TICKS)\n",
    "plt.yticks(fontsize=FONTSIZE_TICKS)\n",
    "#ax.figure.savefig('figures/final_overleaf_definitions_13_09_2022/REFQX2_count={0}.pdf'.format(disorder_count))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrating data for this figure - SKIP THIS SECTION"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# New data format\n",
    "def data_kz_scaling_disorder(sys_size,max_N,model_name,xaxis=None):\n",
    "    # Load data from files\n",
    "    load_filename = 'pt_localization_excitation_L={0}_maxN={1}_system_majorana_mapping_disorder_everystep=True_rad=1_trotter=False_{2}'.format(str(sys_size),str(max_N),model_name)\n",
    "    yaxis_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_mean_dict.npy', allow_pickle=True).item()\n",
    "    yaxis_std_dict = np.load(load_filename+'_endstep_new_nsteps_definition_corrected_noise_data_std_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "    # Choose the xaxis to be the N's of the first in the dictionary\n",
    "    if xaxis is None:\n",
    "        xaxis = list(list(yaxis_dict.values())[0].keys())\n",
    "    \n",
    "    sigmas = []\n",
    "    defects = []\n",
    "    defects_err = []\n",
    "    for k,yaxis in sorted(yaxis_dict.items()):\n",
    "        sigmas.append(k)\n",
    "        l_yaxis = []\n",
    "        l_yerr_arr = []\n",
    "        for n_opt in xaxis:\n",
    "            l_yaxis.append(yaxis[n_opt])\n",
    "            l_yerr_arr.append(yaxis_std_dict[k][n_opt])\n",
    "        defects.append(np.array(l_yaxis))\n",
    "        #defects.append(np.array(l_yaxis)*(sys_size-1)/(sys_size)+(1/(2*sys_size)))\n",
    "        #defects.append(np.array(l_yaxis)+(1/(2*sys_size)))\n",
    "        defects_err.append(np.array(l_yerr_arr))\n",
    "    \n",
    "    return np.array(xaxis),np.array(sigmas),np.array(defects),np.array(defects_err)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "count=5000\n",
    "model_name = 'noise=0.1_disorder=0.1_experimental_sys_sizes_count='+str(count)\n",
    "max_N = 30\n",
    "L_sizes = [4,5,6]\n",
    "for L_size in L_sizes:\n",
    "    xaxis,sigmas,defects,defects_err=data_kz_scaling_disorder(L_size,max_N,model_name)\n",
    "    saved_avg_arr = np.array([np.array(defects[0]),np.array(defects_err[0]/np.sqrt(count))])\n",
    "    np.save(model_name+'_L={0}'.format(str(L_size)),saved_avg_arr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.3(b): Part 1, Varying every step noise, $L = 30,45,60$, $N_{max}= 200$. Fit of $\\sigma^2$ plotted above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = sorted(np.append(1e-5*np.array(range(10,800,50)),0))\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 3000#30\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = [max_N]#list(range(N_start,max_N+1,5))\n",
    "L_sizes = [20,30,40]\n",
    "disorder_count = 5\n",
    "is_disorder_every_step = False\n",
    "model_name = 'noise='+str(disorder_xx_var_arr[0])+'_count='+str(disorder_count)\n",
    "print(N_opt)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_AXIS = 30\n",
    "FONTSIZE_TICKS = 26\n",
    "\n",
    "plot_sat_values_with_fixed_slope_noise(L_sizes,max_N,match_guess=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.3(a): Part 2, Varying noise, $L=20$, $N_{max} = 2000--4000$ for $\\sigma = 0,5e-5,1e-4,5e-4$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0,1e-4,5e-4,1e-3]\n",
    "static_disorder_xx_var_arr = [0]\n",
    "#max_N = 4000\n",
    "#N_start = 2000\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = np.ceil(sorted(np.append(np.logspace(1,4,30),0))).astype(int)\n",
    "L_sizes = [20]\n",
    "disorder_count = 5\n",
    "is_disorder_every_step = False\n",
    "model_name = 'alt_vn_noise='+str(disorder_xx_var_arr[0])\n",
    "print(N_opt)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FONTSIZE_AXIS = 30\n",
    "FONTSIZE_TICKS = 26\n",
    "\n",
    "L_sizes = [20]\n",
    "max_N = 10000\n",
    "model_name = 'alt_vn_noise=0'\n",
    "plot_sat_values_with_fixed_slope_noise_nmax(L_sizes[0],max_N,match_guess=False,cap_nmax=80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NOT NEEDED, SKIP! Fig.3(b): Part 2 - Alternative, Varying noise, $L=20$, $N_{max} = 2000,2500,3000,3500,4000$. Fit of $\\sigma^2$ plotted above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = sorted(np.append(1e-5*np.array(range(10,800,50)),0))\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 4000\n",
    "N_start = 2000\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = [200,800,1400,2800,4000]\n",
    "L_sizes = [20]\n",
    "disorder_count = 5\n",
    "is_disorder_every_step = False\n",
    "model_name = 'varying_noise_alternative'\n",
    "print(N_opt)\n",
    "print(disorder_xx_var_arr)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "L_sizes = [20]\n",
    "max_N = 4000\n",
    "model_name = 'varying_noise_alternative'\n",
    "plot_sat_values_with_fixed_slope_noise_alternative(L_sizes,max_N,match_guess=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig. 7: Plotting the density of exitations versus the number of steps for many system sizes $L=4, 8, 16, 32, 64, 128$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "disorder_xx_var_arr = [0]\n",
    "static_disorder_xx_var_arr = [0]\n",
    "max_N = 120\n",
    "N_start = 1\n",
    "radi = 1\n",
    "to_trotter_radius = False\n",
    "N_opt = list(range(N_start,max_N+1,1))\n",
    "L_sizes = [4,8,16,32,64,128,256]\n",
    "disorder_count = 1 # There is no disorder so we take only one sample\n",
    "model_name = 'no_disorder_many_different_system_sizes'\n",
    "print(N_opt)\n",
    "print(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_disorder_data(L_sizes,N_opt,disorder_xx_var_arr,static_disorder_xx_var_arr,cur_disorder_count=disorder_count,_model_name=model_name,radiu=radi,to_trotter= to_trotter_radius)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "# Load the data\n",
    "max_N =120\n",
    "ax=plot_zero_disorder_match(L_sizes,max_N,model_name,add1overL=False,fit_region=[50,119])\n",
    "#ax.figure.savefig('figures/final_overleaf_definitions_10_06_2022/FIG7.pdf')\n",
    "ax.figure.savefig('figures/final_overleaf_definitions_13_09_2022/FIG7.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qiskit implementation for adiabatic sweeping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# NOW IT USES NEW N_STEPS DEFINITION HERE\n",
    "\n",
    "from qiskit.tools.visualization import plot_histogram\n",
    "from functools import reduce\n",
    "# Import API for cloud computation\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister, execute, IBMQ, transpile\n",
    "# Enable when BasicAer not found\n",
    "#from qiskit import Aer\n",
    "from qiskit.quantum_info import Pauli\n",
    "from qiskit.providers.aer import AerSimulator\n",
    "from qiskit.providers.ibmq import least_busy\n",
    "from qiskit.tools.monitor import job_monitor, backend_monitor, backend_overview\n",
    "from qiskit.providers.aer import noise\n",
    "from qiskit import QuantumRegister, QuantumCircuit, Aer\n",
    "from qiskit.quantum_info import Statevector\n",
    "import os.path\n",
    "from datetime import date\n",
    "from qiskit import Aer, transpile\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Font definitions\n",
    "FONTSIZE_LEGEND = 22\n",
    "FONTSIZE_AXIS = 24\n",
    "FONTSIZE_TITLE = 24\n",
    "FONTSIZE_TICKS = 20\n",
    "\n",
    "### INITIALIZATION ###\n",
    "\n",
    "IS_PBC = True\n",
    "\n",
    "# Number of qubits for the simulation\n",
    "QUBITS_NUM = 16\n",
    "SHOTS = 30000 # We test with 8000 shots and then we will increase it to 30000\n",
    "SIM_NUM = 5\n",
    "# BATCH_SIZE = 5 for the real quantum computer as there is a queue\n",
    "BATCH_SIZE = SIM_NUM\n",
    "\n",
    "FILE_OUT = False\n",
    "# local = simulation without noise\n",
    "RUN_TYPE = 'local' # 'local', 'realqc', 'noise'\n",
    "\n",
    "TODAY = str(date.today())\n",
    "FILENAME = 'counts_z2_1system_dist_'+RUN_TYPE+'_'+TODAY+'_'+str(QUBITS_NUM)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Loading cerdentials\n",
    "IBMQ.load_account()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_ener_from_bit_string(cur_bits):\n",
    "    # cur_bits is an array of integers representing bits\n",
    "    cur_ener = 0\n",
    "    # The energy is with OBC, the minimum energy is -L+1\n",
    "    for i in range(len(cur_bits)-1):\n",
    "        # We add 1 for each domain wall and -1 if there is no domain wall like the Hamiltonian\n",
    "        cur_ener += -1*(1-2*cur_bits[i])*(1-2*cur_bits[i+1])\n",
    "    if IS_PBC:\n",
    "        cur_ener += -1*(1-2*cur_bits[len(cur_bits)-1])*(1-2*cur_bits[0])\n",
    "    return cur_ener\n",
    "def calc_dw_from_bit_string(cur_bits):\n",
    "    # cur_bits is an array of integers representing bits\n",
    "    cur_dw = 0\n",
    "    # The energy is with OBC, the minimum energy is -L+1\n",
    "    for i in range(len(cur_bits)-1):\n",
    "        # We add 1 for each domain wall and -1 if there is no domain wall like the Hamiltonian\n",
    "        cur_dw += 1-(1-2*cur_bits[i])*(1-2*cur_bits[i+1])\n",
    "    if IS_PBC:\n",
    "        cur_dw += 1-(1-2*cur_bits[len(cur_bits)-1])*(1-2*cur_bits[0])\n",
    "    return 0.5*cur_dw\n",
    "def calc_dw_cors_from_bit_string(cur_bits):\n",
    "    # Returns array of <N_0N_i>\n",
    "    # cur_bits is an array of integers representing bits\n",
    "    \n",
    "    dw_at_start = (1-(1-2*cur_bits[0])*(1-2*cur_bits[1]))\n",
    "    # The energy is with OBC, the minimum energy is -L+1\n",
    "    max_dws = len(cur_bits)-1\n",
    "    if IS_PBC:\n",
    "        max_dws = len(cur_bits)\n",
    "    cur_cors = np.zeros(max_dws)\n",
    "    for i in range(max_dws):\n",
    "        # We add the multiplication of domain walls which is 1 if both exist and -1 if one is not and the other does\n",
    "        cur_cors[i] = dw_at_start*(1-(1-2*cur_bits[i])*(1-2*cur_bits[(i+1)%len(cur_bits)]))\n",
    "    return 0.25*np.array(cur_cors)\n",
    "def calc_dw_avgs_from_bit_string(cur_bits):\n",
    "    # Returns array of <N_0N_i>\n",
    "    # cur_bits is an array of integers representing bits\n",
    "    # The energy is with OBC, the minimum energy is -L+1\n",
    "    max_dws = len(cur_bits)-1\n",
    "    if IS_PBC:\n",
    "        max_dws = len(cur_bits)\n",
    "    cur_cors = np.zeros(max_dws)\n",
    "    for i in range(max_dws):\n",
    "        # We add the multiplication of domain walls which is 1 if both exist and -1 if one is not and the other does\n",
    "        cur_cors[i] = 0.5*(1-(1-2*cur_bits[i])*(1-2*cur_bits[(i+1)%len(cur_bits)]))\n",
    "    return np.array(cur_cors)\n",
    "def calc_zzzz_cors_from_bit_string(cur_bits):\n",
    "    # Returns array of Z_1Z_2Z_jZ_{j+1}\n",
    "    # cur_bits is an array of integers representing bits\n",
    "    \n",
    "    dw_at_start = (1-2*cur_bits[0])*(1-2*cur_bits[1])\n",
    "    # The energy is with OBC, the minimum energy is -L+1\n",
    "    max_dws = len(cur_bits)-1\n",
    "    if IS_PBC:\n",
    "        max_dws = len(cur_bits)\n",
    "    cur_cors = np.zeros(max_dws)\n",
    "    for i in range(max_dws):\n",
    "        # We add the multiplication of domain walls which is 1 if both exist and -1 if one is not and the other does\n",
    "        cur_cors[i] = dw_at_start*((1-2*cur_bits[i])*(1-2*cur_bits[(i+1)%len(cur_bits)]))\n",
    "    return np.array(cur_cors)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def zz_int(circ,q,i,j,alpha):\n",
    "    # Choose i->j such that it is in the connectivity direction\n",
    "    circ.cx(q[i],q[j])\n",
    "    circ.rz(2*alpha,q[j])\n",
    "    circ.cx(q[i],q[j])\n",
    "\n",
    "# from helper.py\n",
    "def measure_partial_system(circ, qqubit, cbit):\n",
    "    for i, qi in enumerate(qqubit):\n",
    "        # measure qubit\n",
    "        circ.measure(qi, cbit[i])\n",
    "\n",
    "def prepare_plus_state(circ, q):\n",
    "    \"\"\"\n",
    "    From |0> state to |+> state.\n",
    "\n",
    "    Args:\n",
    "        circ : the quantum circuit\n",
    "        q : the register\n",
    "    \"\"\"\n",
    "    for qi in q:\n",
    "        circ.h(qi)\n",
    "\n",
    "def measure_dw_density(counts):\n",
    "    # NOT USED\n",
    "    '''\n",
    "    Return the expected number of domain walls N_ex = 0.5\\sum_i 1-<Z_iZ_{i+1}>\n",
    "    '''\n",
    "\n",
    "    sys_size = len(list(list(counts.keys())[0]))\n",
    "\n",
    "    # Array for <Z_iZ_{i+1}>\n",
    "    zz_arr = np.zeros(sys_size-1)\n",
    "    \n",
    "    total_shots = 0\n",
    "    for res in counts:\n",
    "        # We reverse for convention: The FIRST index in res[0] is the result of the LAST qubit\n",
    "        qubits_val = list(reversed(res))\n",
    "        qubits_val = [int(qubits_val[i]) for i in range(len(qubits_val))]\n",
    "\n",
    "        total_shots += counts[res]\n",
    "        for i in range(len(qubits_val)-1):\n",
    "            # Adding +1 if the spins are alligned and add -1 if the spins are anti alligned\n",
    "            zz_arr[i] += (1-2*qubits_val[i])*(1-2*qubits_val[i+1])*counts[res]\n",
    "\n",
    "    zz_arr = zz_arr*(1/total_shots)\n",
    "    zz_arr = 0.5*(1-zz_arr)\n",
    "    # Print the excitations\n",
    "    # NOTE: We divide here by L-1\n",
    "    return sum(zz_arr)/(sys_size-1)\n",
    "\n",
    "def measure_dw_density_from_probs(probs):\n",
    "    '''\n",
    "    Return the expected number of domain walls density N_ex = 0.5\\sum_i 1-<Z_iZ_{i+1}>/(L-1)\n",
    "    '''\n",
    "    sys_size = round(math.log2(len(probs)))\n",
    "    bin_form = '0'+str(sys_size)+'b'\n",
    "    exp_dw = 0\n",
    "    for i in range(len(probs)):\n",
    "        cur_l = str(format(i, bin_form))\n",
    "        qubits_val = [int(cur_l[i]) for i in range(len(cur_l))]\n",
    "        exp_dw += calc_dw_from_bit_string(qubits_val)*probs[i]\n",
    "    # NOTE: We divide here by L-1\n",
    "    if not IS_PBC:\n",
    "        return exp_dw/(sys_size-1)\n",
    "    else:\n",
    "        return exp_dw/sys_size\n",
    "\n",
    "def measure_dw_ener_dist(counts):\n",
    "    '''\n",
    "    Return the dist of domain walls domain walls N_ex = 0.5\\sum_i 1-<Z_iZ_{i+1}>\n",
    "    '''\n",
    "\n",
    "    sys_size = len(list(list(counts.keys())[0]))\n",
    "\n",
    "    # Array for <Z_iZ_{i+1}>\n",
    "    zz_arr = np.zeros(sys_size-1)\n",
    "\n",
    "    ret_dict = {}\n",
    "    \n",
    "    total_shots = sum(counts.values())\n",
    "    for res in counts:\n",
    "        # We reverse for convention: The FIRST index in res[0] is the result of the LAST qubit\n",
    "        qubits_val = list(reversed(res))\n",
    "        qubits_val = [int(qubits_val[i]) for i in range(len(qubits_val))]\n",
    "        cur_ener = calc_ener_from_bit_string(qubits_val)\n",
    "        cur_prob = counts[res]/total_shots\n",
    "        ret_dict[res] = (cur_prob,cur_ener)\n",
    "    return ret_dict\n",
    "def measure_dw_dist_from_probs(probs):\n",
    "    '''\n",
    "    Return the dist of domain walls domain walls N_ex = 0.5\\sum_i 1-<Z_iZ_{i+1}>\n",
    "    '''\n",
    "    ret_dict = {}\n",
    "    bin_form = '0'+str(round(math.log2(len(probs))))+'b'\n",
    "    for i in range(len(probs)):\n",
    "        cur_l = str(format(i, bin_form))\n",
    "        qubits_val = [int(cur_l[i]) for i in range(len(cur_l))]\n",
    "        cur_prob = probs[i]\n",
    "        cur_ener = calc_ener_from_bit_string(qubits_val)\n",
    "        ret_dict[cur_l] = (cur_prob,cur_ener)\n",
    "        #print(ret_dict[cur_l])\n",
    "    return ret_dict\n",
    "        \n",
    "    sys_size = len(list(list(counts.keys())[0]))\n",
    "\n",
    "    # Array for <Z_iZ_{i+1}>\n",
    "    zz_arr = np.zeros(sys_size-1)\n",
    "\n",
    "    ret_dict = {}\n",
    "    \n",
    "    total_shots = sum(counts.values())\n",
    "    for res in counts:\n",
    "        # We reverse for convention: The FIRST index in res[0] is the result of the LAST qubit\n",
    "        qubits_val = list(reversed(res))\n",
    "        qubits_val = [int(qubits_val[i]) for i in range(len(qubits_val))]\n",
    "        cur_ener = calc_ener_from_bit_string(qubits_val)\n",
    "        cur_prob = counts[res]/total_shots\n",
    "        ret_dict[res] = (cur_prob,cur_ener)\n",
    "    return ret_dict\n",
    "def measure_dw_int_dist_from_probs(probs,funtype=calc_dw_cors_from_bit_string):\n",
    "    '''\n",
    "    Return the dist of domain walls domain walls N_ex[i] = <Z_iZ_{i+1}Z_iZ_{i+1}>\n",
    "    '''\n",
    "    sys_size = round(math.log2(len(probs)))\n",
    "    bin_form = '0'+str(sys_size)+'b'\n",
    "    exp_dw_int = np.zeros(sys_size)\n",
    "    for i in range(len(probs)):\n",
    "        cur_l = str(format(i, bin_form))\n",
    "        qubits_val = [int(cur_l[i]) for i in range(len(cur_l))]\n",
    "        exp_dw_int += funtype(qubits_val)*probs[i]\n",
    "    # NOTE: We divide here by L-1 for OBC\n",
    "    #if not IS_PBC:\n",
    "    #    return exp_dw/(sys_size-1)\n",
    "    #else:\n",
    "    #    return exp_dw/sys_size\n",
    "    return exp_dw_int"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def apply_floquet(circ,q,alpha,beta):\n",
    "    # Apply X rotations\n",
    "    # Creates exp(-i alpha X_i)\n",
    "\n",
    "    # Let us first apply ZZ and then X\n",
    "\n",
    "    # Apply ZZ\n",
    "    for i in range(len(q)-1):\n",
    "        # Creates exp(-i beta z_i z_{i+1})\n",
    "        zz_int(circ,q,i,i+1,beta)\n",
    "    if IS_PBC:\n",
    "        zz_int(circ,q,len(q)-1,0,beta)\n",
    "    # Apply X\n",
    "\n",
    "    for i in range(len(q)):\n",
    "        # Creates exp(-i alpha X_i)\n",
    "        circ.rx(2*alpha,q[i])\n",
    "def apply_floquet_arr(circ,q,alpha,beta):\n",
    "    # Apply X rotations\n",
    "    # Creates exp(-i alpha X_i)\n",
    "\n",
    "    # Let us first apply ZZ and then X\n",
    "\n",
    "    # Apply ZZ\n",
    "    for i in range(len(q)-1):\n",
    "        # Creates exp(-i beta z_i z_{i+1})\n",
    "        zz_int(circ,q,i,i+1,beta[i])\n",
    "    if IS_PBC:\n",
    "        zz_int(circ,q,len(q)-1,0,beta[len(q)-1])\n",
    "    # Apply X\n",
    "\n",
    "    for i in range(len(q)):\n",
    "        # Creates exp(-i alpha X_i)\n",
    "        circ.rx(2*alpha[i],q[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_circ_measure_prob_dist(circ,simulation_num,batch_size, run_type, file_name):\n",
    "    #dws = [None for i in range(simulation_num)]\n",
    "    #jobs = [[None] for i in range(simulation_num)]\n",
    "    #counts = [[None] for i in range(simulation_num)]\n",
    "\n",
    "    # Running the circuit on quantum computer/simulator\n",
    "\n",
    "    #if RUN_TYPE == 'noise':\n",
    "    #    # Choose a real device to get the noise from\n",
    "    #    device = IBMQ.get_provider().get_backend('ibmq_16_melbourne')\n",
    "    #    coupling_map = device.configuration().coupling_map\n",
    "    #    # TODO: define properties\n",
    "    #   noise_model = noise.device.basic_device_noise_model(properties) # some random errors about the prob. here\n",
    "    #   basis_gates = noise_model.basis_gates\n",
    "    #    #print(basis_gates)\n",
    "    #    #exit()\n",
    "    #    # print(noise_model.as_dict())\n",
    "\n",
    "    #if RUN_TYPE != 'realqc':\n",
    "    #    backend = Aer.get_backend('qasm_simulator')\n",
    "    #else:\n",
    "    #    backend = IBMQ.get_backend('ibmq_16_melbourne')\n",
    "    #    max_credits = 5        # Maximum number of credits to spend on executions.\n",
    "    #    backend_monitor(backend)\n",
    "    backend = Aer.get_backend('aer_simulator_statevector')\n",
    "    outputstate = backend.run(circ).result().get_statevector()\n",
    "    probs = Statevector(outputstate).probabilities()\n",
    "    dw_int = measure_dw_int_dist_from_probs(probs,funtype=calc_dw_cors_from_bit_string)\n",
    "    dw_avg = measure_dw_int_dist_from_probs(probs,funtype=calc_dw_avgs_from_bit_string)\n",
    "    dw_int_minus_avgs = [dw_int[i] - dw_avg[0]*dw_avg[i] for i in range(len(dw_int))]\n",
    "    return measure_dw_dist_from_probs(probs),measure_dw_density_from_probs(probs),dw_int,dw_int_minus_avgs\n",
    "    #completed_exps = 0\n",
    "    #while(completed_exps<simulation_num):\n",
    "    #    # Run all the jobs in parallel\n",
    "    #    for i in range(batch_size):\n",
    "    #        jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS)\n",
    "    #        #if run_type == 'noise':\n",
    "    #        #    jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS, noise_model=noise_model,coupling_map=coupling_map, basis_gates=basis_gates)\n",
    "    #        #elif run_type == 'local':\n",
    "    #        #    jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS) # optimization_level=2\n",
    "    #        #else:\n",
    "    #        #    jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS, max_credits=max_credits)\n",
    "    #\n",
    "    #   for i in range(batch_size):\n",
    "    #       job_monitor(jobs[completed_exps+i])\n",
    "    #        sim_result = jobs[completed_exps+i].result()\n",
    "    #        counts[completed_exps+i] = sim_result.get_counts(outcirc)\n",
    "    #        if FILE_OUT:\n",
    "    #            with open(file_name+'.txt','a+') as f:\n",
    "    #                f.write(str(counts[completed_exps+i]))\n",
    "    #                f.write('\\n')\n",
    "    #        # Measure ENERGY DENSITY DIST.\n",
    "    #        dws[completed_exps+i] = measure_dw_ener_dist(counts[completed_exps+i])\n",
    "    #    completed_exps += batch_size\n",
    "    #return dws\n",
    "\n",
    "def run_circ_measure_dw_probs(circ):\n",
    "    backend = Aer.get_backend('aer_simulator_statevector')\n",
    "    outputstate = backend.run(circ).result().get_statevector()\n",
    "    probs = Statevector(outputstate).probabilities()\n",
    "    return measure_dw_density_from_probs(probs)\n",
    "\n",
    "def run_circ_measure_dw(circ,simulation_num,batch_size, run_type, file_name):\n",
    "    dws = [None for i in range(simulation_num)]\n",
    "    jobs = [[None] for i in range(simulation_num)]\n",
    "    counts = [[None] for i in range(simulation_num)]\n",
    "\n",
    "    # Running the circuit on quantum computer/simulator\n",
    "\n",
    "    if RUN_TYPE == 'noise':\n",
    "        # Choose a real device to get the noise from\n",
    "        device = IBMQ.get_provider().get_backend('ibmq_16_melbourne')\n",
    "        coupling_map = device.configuration().coupling_map\n",
    "        # TODO: define properties\n",
    "        noise_model = noise.device.basic_device_noise_model(properties) # some random errors about the prob. here\n",
    "        basis_gates = noise_model.basis_gates\n",
    "        #print(basis_gates)\n",
    "        #exit()\n",
    "        # print(noise_model.as_dict())\n",
    "\n",
    "    if RUN_TYPE != 'realqc':\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "    else:\n",
    "        backend = IBMQ.get_backend('ibmq_16_melbourne')\n",
    "        max_credits = 5        # Maximum number of credits to spend on executions.\n",
    "        backend_monitor(backend)\n",
    "    completed_exps = 0\n",
    "    while(completed_exps<simulation_num):\n",
    "        # Run all the jobs in parallel\n",
    "        for i in range(batch_size):\n",
    "            if run_type == 'noise':\n",
    "                jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS, noise_model=noise_model,coupling_map=coupling_map, basis_gates=basis_gates)\n",
    "            elif run_type == 'local':\n",
    "                jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS) # optimization_level=2\n",
    "            else:\n",
    "                jobs[completed_exps+i] = execute(outcirc, backend=backend, shots=SHOTS, max_credits=max_credits)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            job_monitor(jobs[completed_exps+i])\n",
    "            sim_result = jobs[completed_exps+i].result()\n",
    "            counts[completed_exps+i] = sim_result.get_counts(outcirc)\n",
    "            if FILE_OUT:\n",
    "                with open(file_name+'.txt','a+') as f:\n",
    "                    f.write(str(counts[completed_exps+i]))\n",
    "                    f.write('\\n')\n",
    "            # Measure DW\n",
    "            dws[completed_exps+i] = measure_dw_density(counts[completed_exps+i])\n",
    "\n",
    "        completed_exps += batch_size\n",
    "    mean = np.mean(dws)\n",
    "    stddev = np.std(dws)\n",
    "    print(\"Mean:\" + str(mean))\n",
    "    print(\"Deviation:\" + str(stddev))\n",
    "    return mean, stddev"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "\n",
    "class GetBetaOption(Enum):\n",
    "    lin_dist_fit = 1\n",
    "    thermal_fit = 2\n",
    "\n",
    "def get_beta(cur_dws,fit_op = GetBetaOption.lin_dist_fit):\n",
    "    beta_arr = []\n",
    "    std_arr = []\n",
    "    for cur_sim in cur_dws:\n",
    "        cur_sim = cur_sim.values()\n",
    "        cur_x = [x[1] for x in cur_sim] # This is the energy\n",
    "        cur_y = [x[0] for x in cur_sim] # This is the probability\n",
    "\n",
    "        if fit_op == GetBetaOption.thermal_fit:\n",
    "            # Prepare array of (E_x,average probability) to fit with Boltmann distribution\n",
    "            blotz_fit_dist = dict()\n",
    "            for cur_res in cur_sim:\n",
    "                cur_e = cur_res[1]\n",
    "                cur_p = cur_res[0]\n",
    "                if not cur_e in blotz_fit_dist:\n",
    "                    blotz_fit_dist[cur_e] = []\n",
    "                blotz_fit_dist[cur_e].append(cur_p)\n",
    "            \n",
    "            dos_num_arr = []\n",
    "            # Averaging the probabilities\n",
    "            for cur_res in blotz_fit_dist:\n",
    "                dos_num_arr.append(len(blotz_fit_dist[cur_res]))\n",
    "                blotz_fit_dist[cur_res] = np.sum(blotz_fit_dist[cur_res])\n",
    "\n",
    "            boltz_x = np.array(sorted(list(blotz_fit_dist.keys())))\n",
    "            boltz_y = np.array([blotz_fit_dist[x] for x in boltz_x])\n",
    "\n",
    "            # Get the boltzmann temprature\n",
    "            fit_region = range(int(len(boltz_x)/2))\n",
    "            #fit_params = scipy.optimize.curve_fit(lambda x,beta,c: c-1*beta*x, boltz_x[fit_region], np.log(boltz_y[fit_region]))\n",
    "            #beta = fit_params[0][0]\n",
    "            # TODSO: Fix this\n",
    "            fit_params = scipy.optimize.curve_fit(lambda x,beta,c: c-1*beta*x, boltz_x[fit_region], np.log([boltz_y[i]/dos_num_arr[i] for i in range(len(boltz_y))])[fit_region])\n",
    "            beta = fit_params[0][0]\n",
    "            c = fit_params[0][1]\n",
    "            std = np.sqrt(np.diag(fit_params[1]))[0]\n",
    "            beta_arr.append(beta)\n",
    "            std_arr.append(std)\n",
    "        elif fit_op == GetBetaOption.lin_dist_fit:\n",
    "            # Prepare array of (E_x,average probability) to fit with Boltmann distribution\n",
    "            boltz_x = []\n",
    "            boltz_y = []\n",
    "            for cur_res in cur_sim:\n",
    "                cur_x = cur_res[1]\n",
    "                cur_y = cur_res[0]\n",
    "                if cur_x < 0:\n",
    "                    boltz_x.append(cur_x)\n",
    "                    boltz_y.append(cur_y)\n",
    "            boltz_x = np.array(boltz_x)\n",
    "            boltz_y = np.array(boltz_y)\n",
    "\n",
    "            p, cov = np.polyfit(boltz_x, np.log(boltz_y), 1,cov=True)  \n",
    "            beta = -1*p[0]\n",
    "            c= p[1]\n",
    "            \n",
    "            std = np.sqrt(np.diag(cov))[0] # standard deviation in beta\n",
    "            beta_arr.append(beta)\n",
    "            std_arr.append(std)\n",
    "    return np.average(np.array(beta_arr)),np.max(np.array(std_arr))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PLOTS\n",
    "class DWType(Enum):\n",
    "    zz_cors = 1\n",
    "    dw_int = 2\n",
    "    dw_int_minus_avg = 3\n",
    "# Plot the saturation values as a function of the disorder for static disorder\n",
    "def plot_ener_dist_space(cur_dws,fit_op=GetBetaOption.thermal_fit,noise_model=None):\n",
    "    # Initialize the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$E$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_ylabel(r'$|\\langle x | \\psi \\rangle |^2$',fontsize=FONTSIZE_AXIS)\n",
    "    \n",
    "    # Set log scale\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for cur_sim in cur_dws:\n",
    "        cur_sim = cur_sim.values()\n",
    "        cur_x = [x[1] for x in cur_sim] # This is the energy\n",
    "        cur_y = [x[0] for x in cur_sim] # This is the probability\n",
    "\n",
    "        if fit_op == GetBetaOption.thermal_fit:\n",
    "\n",
    "            # Prepare array of (E_x,average probability) to fit with Boltmann distribution\n",
    "            blotz_fit_dist = dict()\n",
    "            for cur_res in cur_sim:\n",
    "                cur_e = cur_res[1]\n",
    "                cur_p = cur_res[0]\n",
    "                if not cur_e in blotz_fit_dist:\n",
    "                    blotz_fit_dist[cur_e] = []\n",
    "                blotz_fit_dist[cur_e].append(cur_p)\n",
    "            \n",
    "            # Averaging the probabilities\n",
    "            dos_num_arr = []\n",
    "            avg_prob = []\n",
    "            for cur_res in blotz_fit_dist:\n",
    "                dos_num_arr.append(len(blotz_fit_dist[cur_res]))\n",
    "                #print(blotz_fit_dist[cur_res])\n",
    "                avg_prob.append(np.average(blotz_fit_dist[cur_res]))\n",
    "                blotz_fit_dist[cur_res] = np.sum(blotz_fit_dist[cur_res])\n",
    "                \n",
    "            boltz_x = np.array(sorted(list(blotz_fit_dist.keys())))\n",
    "            dos_num_theory = [2*math.comb(-1*min(boltz_x),i)  for i in range(len(boltz_x))]\n",
    "\n",
    "            print(dos_num_arr)\n",
    "            print(dos_num_theory)\n",
    "            print(boltz_x)\n",
    "            boltz_y = np.array([blotz_fit_dist[x] for x in boltz_x])\n",
    "\n",
    "            print(np.sum(boltz_y))\n",
    "            #ax.scatter(cur_x,cur_y)\n",
    "            ax.plot(boltz_x,boltz_y,ls='-',lw=4)\n",
    "\n",
    "            # Get the boltzmann temprature\n",
    "            fit_region = range(int(1*len(boltz_x)/2))\n",
    "            # -1*min(boltz_x)+1 is the system size L\n",
    "            #print(-1*min(boltz_x)+1) = L\n",
    "            #print(len(boltz_y)) = L\n",
    "            #print(np.log([boltz_y[i]/math.comb(-1*min(boltz_x)+1,i+1) for i in range(len(boltz_y))]))\n",
    "            fit_params = scipy.optimize.curve_fit(lambda x,beta,c: c-1*beta*x, boltz_x[fit_region], np.log([boltz_y[i]/dos_num_arr[i] for i in range(len(boltz_y))])[fit_region])\n",
    "            beta = fit_params[0][0]\n",
    "            c = fit_params[0][1]\n",
    "            #percent_error = (a+err)/a-1\n",
    "            print(\"Prefactor:{0}\".format(beta))\n",
    "            # See also https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html:\n",
    "            # To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov)).\n",
    "            print('standard deviation={0}'.format(str(np.sqrt(np.diag(fit_params[1])))))\n",
    "            fit_yaxis = [dos_num_arr[i]*np.exp(c-1*beta*boltz_x[i]) for i in range(len(boltz_x))]\n",
    "            ax.plot(boltz_x,fit_yaxis,ls=\"--\",lw=4,label=r\"$\\beta={0}$\".format(str(beta)))\n",
    "            ax.plot(boltz_x,np.array(avg_prob),lw=4)\n",
    "            print(fit_yaxis[0])\n",
    "            print(boltz_y[0])\n",
    "\n",
    "            load_filename = FILENAME+'_plot_ener_dist_stepstotal='+str(N_steps)+'_noise_model='+str(noise_model)+'_thermal_fit'\n",
    "\n",
    "        elif fit_op == GetBetaOption.lin_dist_fit:\n",
    "            # Prepare array of (E_x,average probability) to fit with Boltmann distribution\n",
    "            boltz_x = []\n",
    "            boltz_y = []\n",
    "            for cur_res in cur_sim:\n",
    "                cur_x = cur_res[1]\n",
    "                cur_y = cur_res[0]\n",
    "                if cur_x < 0:\n",
    "                    boltz_x.append(cur_x)\n",
    "                    boltz_y.append(cur_y)\n",
    "            boltz_x = np.array(boltz_x)\n",
    "            boltz_y = np.array(boltz_y)\n",
    "            \n",
    "            p, cov = np.polyfit(boltz_x, np.log(boltz_y), 1,cov=True)  \n",
    "            beta = -1*p[0]\n",
    "            c= p[1]\n",
    "            \n",
    "            std = np.sqrt(np.diag(cov))[0] # standard deviation in beta\n",
    "\n",
    "            fit_xaxis = np.unique(np.array(boltz_x))\n",
    "            fit_yaxis = [c-beta*fit_xaxis[i] for i in range(len(fit_xaxis))]\n",
    "            ax.scatter(boltz_x,boltz_y)\n",
    "            print(fit_xaxis)\n",
    "            print(fit_yaxis)\n",
    "            ax.plot(np.array(fit_xaxis),np.exp(np.array(fit_yaxis)),lw=4,c=\"red\")\n",
    "\n",
    "            load_filename = FILENAME+'_plot_ener_dist_stepstotal='+str(N_steps)+'_dist_fit'\n",
    "    #if match_guess:\n",
    "    #    load_filename += '_coe='+str(guess_coe)\n",
    "    if IS_PBC:\n",
    "        load_filename += \"_pbc\"\n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')\n",
    "# Plot the domain wall interaction\n",
    "def plot_dw_int(cur_dw_int_dict,steps_arr=None,plot_type=DWType.zz_cors):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$r$',fontsize=FONTSIZE_AXIS)\n",
    "    if plot_type == DWType.zz_cors:\n",
    "        ax.set_ylabel(r'$\\langle Z_0Z_1Z_{r}Z_{r+1} \\rangle $',fontsize=FONTSIZE_AXIS)\n",
    "    elif plot_type == DWType.dw_int:\n",
    "        ax.set_ylabel(r'$\\langle N_0 N_{r} \\rangle $',fontsize=FONTSIZE_AXIS)\n",
    "    elif plot_type == DWType.dw_int_minus_avg:\n",
    "        ax.set_ylabel(r'$\\langle N_0 N_{r} \\rangle - \\langle N_0 \\rangle \\langle N_{r} \\rangle$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "    # Set log scale\n",
    "    #ax.set_yscale('log')\n",
    "\n",
    "    steps_keys = list(cur_dw_int_dict.keys())\n",
    "    steps_keys_len = len(steps_keys)\n",
    "    sys_size = len(cur_dw_int_dict[steps_keys[0]])\n",
    "\n",
    "    xaxis = np.array(list(range(sys_size)))[1:]\n",
    "\n",
    "    if steps_arr is None:\n",
    "        steps_arr = [steps_keys[i] for i in range(0,steps_keys_len,int(steps_keys_len/10))]\n",
    "    for s in steps_arr:\n",
    "        yaxis = np.array(cur_dw_int_dict[s])[1:]\n",
    "        ax.plot(xaxis,yaxis,lw=4,marker='o',label=r'$N_{{\\rm steps}}={0}$'.format(str(s)))\n",
    "\n",
    "    load_filename = FILENAME+'_plot_dw_int_stepstotal='+str(max(list(cur_dw_int_dict.keys())))+'_steps_arr={0}_type={1}'.format(str(steps_arr),str(plot_type.name))\n",
    "    plt.tight_layout()\n",
    "    if IS_PBC:\n",
    "        load_filename += \"_pbc\"\n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='lower right',fontsize=FONTSIZE_LEGEND)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')\n",
    "def plot_cor_mid(cur_dw_int_dict,steps_arr=None,plot_type=DWType.dw_int_minus_avg):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    #ax.set_ylabel(r'Excitations $N_{\\rm ex}$',fontsize=FONTSIZE_AXIS)\n",
    "    ax.set_xlabel(r'$N_{\\mathrm{steps}}$',fontsize=FONTSIZE_AXIS)\n",
    "    #if plot_type == DWType.zz_cors:\n",
    "    #    ax.set_ylabel(r'$\\langle Z_0Z_1Z_{r}Z_{r+1} \\rangle $',fontsize=FONTSIZE_AXIS)\n",
    "    #elif plot_type == DWType.dw_int:\n",
    "    #    ax.set_ylabel(r'$\\langle N_0 N_{r} \\rangle $',fontsize=FONTSIZE_AXIS)\n",
    "    if plot_type == DWType.dw_int_minus_avg:\n",
    "        ax.set_ylabel(r'$\\langle N_0 N_{L/2} \\rangle - \\langle N_0 \\rangle \\langle N_{L/2} \\rangle$',fontsize=FONTSIZE_AXIS)\n",
    "    else:\n",
    "        raise Exception(\"TO DO\")\n",
    "    # Set log scale\n",
    "    #ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    steps_keys = list(cur_dw_int_dict.keys())\n",
    "    steps_keys_len = len(steps_keys)\n",
    "    sys_size = len(cur_dw_int_dict[steps_keys[0]])\n",
    "\n",
    "\n",
    "    xaxis = np.array(steps_keys[:])\n",
    "    yaxis = np.array([np.array(cur_dw_int_dict[s])[int(math.floor(sys_size/2))] for s in xaxis])\n",
    "    ax.plot(xaxis,yaxis,lw=4,marker='o')\n",
    "\n",
    "    # Add simple linear fit\n",
    "    fit_len = 10\n",
    "    fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, xaxis[:fit_len], np.log(yaxis)[:fit_len])\n",
    "    fit_slope = fit_params[0][1]\n",
    "    fit_shift = fit_params[0][0]\n",
    "    ax.plot(xaxis,np.exp(fit_shift+fit_slope*xaxis),c=\"red\",lw=3,label=r\"$e^{{{0}+{1}N_{{\\mathrm{{steps}}}}}}$\".format(round(fit_shift,3),round(fit_slope,3)))\n",
    "\n",
    "    load_filename = FILENAME+'_plot_cor_mid_stepstotal='+str(max(list(cur_dw_int_dict.keys())))\n",
    "    plt.tight_layout()\n",
    "    if IS_PBC:\n",
    "        load_filename += \"_pbc\"\n",
    "    # Change tick sizes\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "    \n",
    "    ax.legend(loc='upper right',fontsize=FONTSIZE_LEGEND+4)\n",
    "    ax.figure.savefig(load_filename+'.pdf', bbox_inches='tight')    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_dict_sim(cur_nsteps,sys_size,choose_steps=None,noise_model=None):\n",
    "    if choose_steps is None:\n",
    "        choose_steps = cur_nsteps-7\n",
    "    QUBITS_NUM = sys_size\n",
    "    TODAY = str(date.today())\n",
    "    FILENAME = r'C:\\Users\\USER\\Desktop\\Backup\\code_backup_dms\\KZ_sim'+'\\\\counts_z2_1system_dist_'+RUN_TYPE+'_'+str(QUBITS_NUM)\n",
    "    print(FILENAME)\n",
    "    try:\n",
    "        fname_toload = FILENAME+'_'+str(choose_steps)+'_stepstotal='+str(cur_nsteps)+\".npy\"\n",
    "        cur_resdws_dict = np.load(fname_toload,allow_pickle=True).item()\n",
    "    except Exception:\n",
    "        fname_toload = FILENAME+'_'+str(choose_steps)+'_stepstotal='+str(cur_nsteps)+'_noise_model='+str(noise_model)+\".npy\"\n",
    "        cur_resdws_dict = np.load(fname_toload,allow_pickle=True).item()       \n",
    "    return cur_resdws_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Move to imports\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# TODO: ADD HERE THE CODE FOR CALCULATING EXPLICITELY THE PROBABILITY OF <s_i|\\psi> using ED and qiskit. Plot graph.\n",
    "# Run the adiabatic sweeping\n",
    "# We verfied that this definition matches the L=4 verfication_results\n",
    "\n",
    "@dataclass\n",
    "class NoiseParameters:\n",
    "    each_step_noise: float\n",
    "    disorder: float\n",
    "    \n",
    "def get_beta_arr(N_steps,sys_size,skip_steps=20,noise_model=None):\n",
    "    global QUBITS_NUM\n",
    "    global FILENAME\n",
    "    global TODAY\n",
    "    QUBITS_NUM = sys_size\n",
    "    TODAY = str(date.today())\n",
    "    FILENAME = 'counts_z2_1system_dist_'+RUN_TYPE+'_'+str(sys_size)\n",
    "    r0=1 # For some reason we use 0.5 factor in the free fermion definitions\n",
    "    betas_thermal = dict()\n",
    "    betas_dist = dict()\n",
    "    beta_from_dw_arr = dict()\n",
    "    dw_int_dict = dict()\n",
    "    dw_int_minus_avg_dict = dict()\n",
    "\n",
    "    # Initializing disorder noise to be the same for all steps\n",
    "    if noise_model:\n",
    "        disorder_alpha = np.random.normal(0, noise_model.disorder, sys_size)\n",
    "        disorder_beta = np.random.normal(0, noise_model.disorder, sys_size)\n",
    "\n",
    "    for steps in range(3,N_steps+1,skip_steps):\n",
    "        #if steps<N_steps:\n",
    "        #    continue\n",
    "        # Create a Quantum Register with QUBITS_SYSTEM_NUM>?QUBITS_NUM qubits.\n",
    "        regq = QuantumRegister(sys_size)\n",
    "        c = ClassicalRegister(sys_size)\n",
    "        # Create a Quantum Circuit acting on the q register\n",
    "        outcirc = QuantumCircuit(regq, c)\n",
    "\n",
    "        # Prepare the ground state of the initial state, which is the X basis\n",
    "        prepare_plus_state(outcirc,regq)\n",
    "\n",
    "        # New N_steps definition - Copy pasted rom the free-fermion method\n",
    "        steps_list = np.linspace(0,np.pi/2,steps+2)[1:-1]\n",
    "\n",
    "        # Evolution with F(alpha,beta) adiabatically for N_{steps}\n",
    "        for cur_step in steps_list:\n",
    "            theta = cur_step\n",
    "            # The half here seems to be artifact of some notation\n",
    "            # TODO: ADD NOISE HERE\n",
    "            alpha_cur = r0 * np.cos(theta) # H_{PM} coefficient\n",
    "            beta_cur = r0*np.sin(theta) # H_{FM} coefficient\n",
    "            if noise_model:\n",
    "                alpha_arr = np.zeros(sys_size)\n",
    "                if not IS_PBC:\n",
    "                    beta_arr = np.zeros(sys_size-1)\n",
    "                else:\n",
    "                    beta_arr = np.zeros(sys_size)\n",
    "                # Noise\n",
    "                alpha_arr += np.random.normal(0, noise_model.each_step_noise, sys_size)\n",
    "                beta_arr += np.random.normal(0, noise_model.each_step_noise, sys_size)\n",
    "                # Disorder\n",
    "                alpha_arr += disorder_alpha\n",
    "                beta_arr += disorder_beta\n",
    "                apply_floquet_arr(outcirc,regq,alpha_arr+alpha_cur,beta_arr+beta_cur)\n",
    "            # TODO: Make a version for apply_floquet with array instead of one variable\n",
    "            else:\n",
    "                apply_floquet(outcirc,regq,alpha_cur,beta_cur)\n",
    "        outcirc.save_statevector()\n",
    "        #measure_partial_system(outcirc,regq, c)\n",
    "        fname_save = FILENAME+'_'+str(steps)+'_stepstotal='+str(N_steps)+'_noise_model='+str(noise_model)\n",
    "        res_dws,dw_exp,dw_int,dw_int_minus_avg = run_circ_measure_prob_dist(outcirc,SIM_NUM,BATCH_SIZE, RUN_TYPE, fname_save)\n",
    "        # Save the array first\n",
    "        np.save(r\"C:\\Users\\USER\\Desktop\\Backup\\code_backup_dms\\KZ_sim\"+\"\\\\\"+fname_save+\".npy\", res_dws)\n",
    "        cur_thermal_beta_pair = get_beta([res_dws],fit_op = GetBetaOption.thermal_fit)\n",
    "        cur_dist_beta_pair = get_beta([res_dws],fit_op = GetBetaOption.lin_dist_fit)\n",
    "        betas_thermal[steps] = cur_thermal_beta_pair\n",
    "        betas_dist[steps] = cur_dist_beta_pair\n",
    "        dw_int_dict[steps] = dw_int\n",
    "        dw_int_minus_avg_dict[steps] = dw_int_minus_avg\n",
    "        print(cur_thermal_beta_pair)\n",
    "        print(cur_dist_beta_pair)\n",
    "\n",
    "        beta_from_dw_arr[steps] = -0.5*np.log(dw_exp/(1-dw_exp))\n",
    "        \n",
    "        \n",
    "        #plot_ener_dist_space([res_dws])\n",
    "    return betas_thermal,betas_dist,beta_from_dw_arr,dw_int_dict,dw_int_minus_avg_dict\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "noise_pa = NoiseParameters(each_step_noise=0.0,disorder=0)\n",
    "glob_N_steps = 12\n",
    "sys_size_arr = [14]#[20]\n",
    "dw_res_dist_arr = [(x,get_beta_arr(glob_N_steps,x,1,noise_pa)) for x in sys_size_arr]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot \\beta(N_steps) with varying N_steps and constant system size\n",
    "glob_N_steps = 500\n",
    "sys_size_arr = [14]#[20]\n",
    "dw_res_dist_arr = [(x,get_beta_arr(glob_N_steps,x,20)) for x in sys_size_arr]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#steps_choice = range(3,50,4)\n",
    "steps_choice = None\n",
    "plot_dw_int(dw_res_dist_arr[0][1][3],steps_choice,plot_type=DWType.dw_int)\n",
    "plot_dw_int(dw_res_dist_arr[0][1][4],steps_choice,plot_type=DWType.dw_int_minus_avg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_cor_mid(dw_res_dist_arr[0][1][4],steps_choice,plot_type=DWType.dw_int_minus_avg)\n",
    "# Clearly exponential fit should work here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N_steps=3\n",
    "print(sys_size_arr[0])\n",
    "loaded_sim_dict = get_dict_sim(glob_N_steps,sys_size_arr[0],N_steps,noise_pa)\n",
    "print(len(list(loaded_sim_dict.keys())[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_ener_dist_space([loaded_sim_dict],GetBetaOption.thermal_fit,noise_pa)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This is the plot for \\beta(N_steps)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "syssize_arr = [x[0] for x in dw_res_dist_arr]\n",
    "for cur_sys_size,cur_betas in dw_res_dist_arr:\n",
    "    if cur_sys_size<15:\n",
    "        continue\n",
    "    cur_thermal_betas = cur_betas[0]\n",
    "    cur_dist_betas = cur_betas[1]\n",
    "    cur_dw_betas = cur_betas[2]\n",
    "    xaxis = sorted(cur_thermal_betas.keys())[1:]\n",
    "    yaxis_thermal = [cur_thermal_betas[x][0] for x in xaxis]\n",
    "    yerr_thermal_arr = [cur_thermal_betas[x][1] for x in xaxis]\n",
    "    yaxis_dist = [cur_dist_betas[x][0] for x in xaxis]\n",
    "    yerr_dist_arr = [cur_dist_betas[x][1] for x in xaxis]\n",
    "    yaxis_dw = [cur_dw_betas[x] for x in xaxis]\n",
    "    #print(dw)\n",
    "    #yaxis_dw = [np.log(np.exp(-2*cur_dw_betas[x])/(1-np.exp(-2*cur_dw_betas[x])))/(-2) for x in xaxis]\n",
    "    \n",
    "\n",
    "    fit_params = scipy.optimize.curve_fit(lambda x,a,b: a+b*x, np.log(np.array(xaxis[:])[:]), np.log(np.array(yaxis_dist[:])[:]))\n",
    "    a = fit_params[0][0]\n",
    "    b = fit_params[0][1]\n",
    "    #print('yaxis:'+str(np.log(np.array(yaxis[:])[:])))\n",
    "    print('{0}+{1}*x'.format(str(round(a,5)), str(round(b,5))))\n",
    "    \n",
    "    ax.errorbar(xaxis,yaxis_thermal,ls='--',marker='o',markersize=4,capsize=2,yerr=yerr_thermal_arr,label=r\"$L={0}$ from thermal fit\".format(cur_sys_size))\n",
    "    ax.errorbar(xaxis,yaxis_dist,ls='--',marker='o',markersize=4,capsize=2,yerr=yerr_dist_arr,label=r\"$L={0}$ from dist. fit\".format(cur_sys_size))\n",
    "    ax.plot(xaxis,yaxis_dw,ls='-',marker='o',markersize=4,label=r\"$L={0}$ from DW fit\".format(cur_sys_size))\n",
    "    \n",
    "    xaxis_fit = np.log(np.array(xaxis[:])[:])\n",
    "    #ax.plot(xaxis,np.exp(a+b*xaxis_fit),ls='-',markersize=4)\n",
    "    if cur_sys_size==20:\n",
    "        ax.plot(xaxis,0.01+0.5*np.log(xaxis),ls='-',lw=3)\n",
    "# Set log scale\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax.set_ylabel(r'$\\beta$',fontsize=FONTSIZE_AXIS)\n",
    "ax.set_xlabel(r'$N_{\\rm steps}$',fontsize=FONTSIZE_AXIS)\n",
    "# TODO: Unique names\n",
    "name = 'qiskit_beta_as_function_of_nsteps_L={0}'.format(str(syssize_arr))\n",
    "if IS_PBC:\n",
    "    name += \"_pbc\"\n",
    "\n",
    "# Change tick sizes\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = FONTSIZE_TICKS)\n",
    "\n",
    "ax.legend(loc='upper left',fontsize=FONTSIZE_LEGEND)\n",
    "ax.figure.savefig(name+'.png')\n",
    "ax.figure.savefig(name+'.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph here shows the $\\beta$ is not the same as the distribution is not exactly termal. Actually references show that the distribution of kinks is approximaetly normal. See papers by DWave and Adolfo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_i P(Z_iZ_{i+1}=1,Z_{j\\neq i}Z_{j+1}=-1) = \\sum_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum simulation of the domain walls defects"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sys_size = 16"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the adiabatic sweeping\n",
    "# We verfied that this definition matches the L=4 verfication_results\n",
    "N_steps = 800\n",
    "#dt = 0.1\n",
    "r0=1 # For some reason we use 0.5 factor in the free fermion definitions\n",
    "\n",
    "means = []\n",
    "devs = []\n",
    "\n",
    "for steps in range(1,N_steps+1,20):\n",
    "    # Create a Quantum Register with QUBITS_SYSTEM_NUM>?QUBITS_NUM qubits.\n",
    "    regq = QuantumRegister(sys_size)\n",
    "    c = ClassicalRegister(sys_size)\n",
    "    # Create a Quantum Circuit acting on the q register\n",
    "    outcirc = QuantumCircuit(regq, c)\n",
    "\n",
    "    # Prepare the ground state of the initial state, which is the X basis\n",
    "    prepare_plus_state(outcirc,regq)\n",
    "    \n",
    "    #curT = 0.5*dt*steps\n",
    "    # New N_steps definition - Copy pasted rom the free-fermion method\n",
    "    steps_list = np.linspace(0,np.pi/2,steps+2)[1:-1]\n",
    "\n",
    "    # Evolution with F(alpha,beta) adiabatically for N_{steps}\n",
    "    for cur_step in steps_list:\n",
    "        #curg = -1*(2-(cur_step*dt)/curT)\n",
    "        #curJ = -1*(cur_step*dt)/curT\n",
    "        #s = float(cur_step_num)/float(N_steps)\n",
    "        #alpha_cur = curg*dt # H_{PM} coefficient\n",
    "        #beta_cur =  curJ*dt # H_{FM} coefficient\n",
    "        theta = cur_step\n",
    "        # NO HALF HERE AS WE CORRECTED THE NOTATION\n",
    "        alpha_cur = r0 * np.cos(theta) # H_{PM} coefficient\n",
    "        beta_cur = r0*np.sin(theta) # H_{FM} coefficient\n",
    "        apply_floquet(outcirc,regq,alpha_cur,beta_cur)\n",
    "\n",
    "    # Measure and get from it the domain wall excitations\n",
    "    #measure_partial_system(outcirc,regq, c)\n",
    "    #outcirc.draw()\n",
    "    #mean, stddev = run_circ_measure_dw(outcirc,SIM_NUM,BATCH_SIZE, RUN_TYPE, FILENAME+'_'+str(steps)+'_stepstotal='+str(N_steps))\n",
    "    #means.append(mean)\n",
    "    #devs.append(stddev)\n",
    "\n",
    "    outcirc.save_statevector()\n",
    "    res = run_circ_measure_dw_probs(outcirc)\n",
    "    means.append(res)\n",
    "    devs.append(0)\n",
    "\n",
    "print(means)\n",
    "print(devs)\n",
    "#apply_floquet(withf_circ,regq,alpha_cur,beta_cur)\n",
    "#withf_circ.draw()\n",
    "#run_circ(withf_circ,SIM_NUM,BATCH_SIZE, RUN_TYPE, FILENAME+'_afterf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot DW density graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xaxis = list(range(2,N_steps+1,20))\n",
    "ax.set_xticks(xaxis) # NO MINOR BOOL FOR NEWER MATPLOTLIB\n",
    "ax.set_ylabel(r'Excitations density $\\frac{N_{\\rm ex}}{L}$',fontsize=FONTSIZE_AXIS)\n",
    "ax.set_xlabel(r'$N_{\\rm steps}$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "# Set log scale\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')  \n",
    "\n",
    "l_yaxis = means\n",
    "l_yerr_arr = devs\n",
    "#l_yerr_arr = np.zeros(len(l_yerr_arr))\n",
    "ax.errorbar(xaxis,l_yaxis,ls='--',label=r'$\\sigma=0$',marker='o',markersize=4,capsize=2,yerr=l_yerr_arr)\n",
    "\n",
    "print(xaxis[-1])\n",
    "print(l_yaxis[-1])\n",
    "\n",
    "# TODO: Unique names\n",
    "name = 'kz_scaling_'+str(QUBITS_NUM)+'system_qiskit_debug_to_delete'\n",
    "\n",
    "# Change tick sizes\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = FONTSIZE_TICKS)\n",
    "\n",
    "ax.legend(loc='lower left',fontsize=FONTSIZE_LEGEND)\n",
    "#ax.figure.savefig(name+'.png')\n",
    "#ax.figure.savefig(name+'.svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the matching $\\beta$ graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xaxis = list(range(2,N_steps+1,20))\n",
    "#ax.set_xticks(xaxis) # NO MINOR BOOL FOR NEWER MATPLOTLIB\n",
    "ax.set_ylabel(r'$\\beta$',fontsize=FONTSIZE_AXIS)\n",
    "ax.set_xlabel(r'$N_{\\rm steps}$',fontsize=FONTSIZE_AXIS)\n",
    "\n",
    "# Set log scale\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xscale('log')  \n",
    "\n",
    "# Assuming \\beta is large enough\n",
    "l_yaxis = np.log(means)/(-2)\n",
    "l_yerr_arr = devs\n",
    "#l_yerr_arr = np.zeros(len(l_yerr_arr))\n",
    "ax.errorbar(xaxis,l_yaxis,ls='--',label=r'$\\sigma=0$',marker='o',markersize=4,capsize=2,yerr=l_yerr_arr)\n",
    "\n",
    "print(xaxis[-1])\n",
    "print(l_yaxis[-1])\n",
    "\n",
    "# TODO: Unique names\n",
    "name = 'kz_scaling_'+str(QUBITS_NUM)+'system_qiskit_debug_to_delete'\n",
    "\n",
    "# Change tick sizes\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = FONTSIZE_TICKS)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = FONTSIZE_TICKS)\n",
    "\n",
    "ax.legend(loc='lower left',fontsize=FONTSIZE_LEGEND)\n",
    "#ax.figure.savefig(name+'.png')\n",
    "#ax.figure.savefig(name+'.svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d417a21d97a2e6b8832d21dc44f5cbe4be6e18325f874a8bb8484565b928cd54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
